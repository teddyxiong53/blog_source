---
title: npu之音频分类
date: 2021-09-08 10:58:33
tags:
	- npu

---

--

从本质上说，音频分类的性能依赖于音频中的特征提取。

传统特征提取算法使用音频特征的统计信息作为分类的依据,

使用到的音频特征包括线性预测编码、短时平均能量等。

近年来，基于深度学习的音频分类取得了较大进展。

基于端到端的特征提取方式，**深度学习可以避免繁琐的人工特征设计。**

音频的多样化给“机器听觉”带来了巨大挑战。

如何对音频信息进行有效的分类,

从繁芜丛杂的数据集中将具有某种特定形态的音频归属到同一个集合，

对于学术研究及工业应用具有重要意义。

对数据的进一步说明：

train.zip中包含三十个文件夹，

每个文件夹的名称是里面所有音频文件的标签，

音频文件的格式是wav格式，时间长度都小于等于1s。

test.zip中包含所有测试集的音频文件，一共有6835个。





# 音频相关的特征

在任意一个Automatic speech recognition 系统中，

第一步就是提取特征。

换句话说，我们需要把音频信号中具有辨识性的成分提取出来，

然后把其他的乱七八糟的信息扔掉，

例如背景噪声啊，情绪啊等等。



## 声谱图（Spectrogram）

我们处理的是语音信号，那么如何去描述它很重要。

因为不同的描述方式放映它不同的信息。

那怎样的描述方式才利于我们观测，利于我们理解呢？

这里我们先来了解一个叫声谱图的东西。

这里，这段语音被分为很多帧，

每帧语音都对应于一个频谱（通过短时FFT计算），

频谱表示频率与能量的关系。

在实际使用中，频谱图有三种，即线性振幅谱、对数振幅谱、自功率谱

（对数振幅谱中各谱线的振幅都作了对数计算，所以其纵坐标的单位是dB（分贝）。这个变换的目的是使那些振幅较低的成分相对高振幅成分得以拉高，以便观察掩盖在低幅噪声中的周期信号）。



声谱图也叫语谱图。

语谱图的x是时间，

y轴是频率，

z轴是幅度。

**幅度用亮色如红色表示高，用深色表示低。**

利用语谱图可以查看指定频率端的能量分布。



这个领域的算法库一般有librosa、essentia、torchaudio、深度学习等。这个领域目前的工程套路是：

1. 首先声音是一维的时域信号，但是计算机看了觉得没啥用（你看wav文件那些采样点，这些数字能说明啥呀）；P.S. 人的听觉系统（从耳朵到大脑皮层）与之相比是多么强大呀！
2. 既然一维的时域信号没啥用，于是人们开始做频域分析，也就是大名鼎鼎的fft；终于有了一些用处了，但是还是差的太远；
3. 把频域和时域都加上，比如stft，最经典的时频域分析方法，怎么样？嗯，又厉害了些；
4. 把时频域的分析结果转化成热力图，送给以图像分析见长的CNN网络怎么样？哇，CNN相当于人类听觉系统中的大脑皮层了，又厉害了好多；
5. 但是和各种声音分析的任务比起来，还是差的比较多；那怎么弥补呢？两个方面，一是更高效的提取出声音的有效特征；二是深度学习算法的发展。在声音人工提取特征方面，几十年来进展不多，我们自然而然就放弃了；剩下的就只能仰仗深度学习算法了。



声音转换为声谱图，也是研究声音特征的一种方法。简单来说，就是将声音通过短时傅立叶变换，获得一张包含3个维度数据的热力图。



https://www.jianshu.com/p/4c23adec3165

## 平均过零率

该特征在语音识别和音乐信息检索中都被大量的使用，对于高冲击声，它通常具有更高的值

## 平均光谱质心

谱质心(Spectral Centroid)是描述音色属性的重要物理参数之一，是频率成分的重心，是在一定频率范围内通过能量加权平均的频率，其单位是Hz。它是声音信号的频率分布和能量分布的重要信息。在主观感知领域，谱质心描述了声音的明亮度，具有阴暗、低沉品质的声音倾向有较多低频内容，谱质心相对较低，具有明亮、欢快品质的多数集中在高频，谱质心相对较高。该参数常用于对乐器声色的分析研究。

## 梅尔频率倒谱系数

它是一小组特征（通常10-40），其简明地描述了**频谱包络**的整体形状

搞清语音是怎么产生的对于我们理解语音有很大帮助。

人通过声道产生声音，声道的shape（形状？）决定了发出怎样的声音。

声道的shape包括舌头，牙齿等。

如果我们可以准确的知道这个形状，那么我们就可以对产生的音素phoneme进行准确的描述。

声道的形状在语音短时功率谱的包络中显示出来。

而MFCCs就是一种准确描述这个包络的一种特征。

MFCCs（Mel Frequency Cepstral Coefficents）是一种在自动语音和说话人识别中广泛使用的特征。

它是在1980年由Davis和Mermelstein搞出来的。

从那时起。在语音识别领域，MFCCs在人工特征方面可谓是鹤立鸡群，一枝独秀，从未被超越啊（至于说Deep Learning的特征学习那是后话了）。



那我们为什么要在声谱图中表示语音呢？

首先，音素（Phones）的属性可以更好的在这里面观察出来。

另外，通过观察共振峰和它们的转变可以更好的识别声音。

隐马尔科夫模型（Hidden Markov Models）就是隐含地对声谱图进行建模以达到好的识别性能。

还有一个作用就是它可以直观的评估TTS系统（text to speech）的好坏，

直接对比合成的语音和自然的语音声谱图的匹配度即可。

下面是一个语音的频谱图。峰值就表示语音的主要频率成分，我们把这些峰值称为共振峰（formants），而共振峰就是携带了声音的辨识属性（就是个人身份证一样）。所以它特别重要。用它就可以识别不同的声音。

![img](../images/random_name/20130623210115156)



既然它那么重要，那我们就是需要把它提取出来！

我们要提取的不仅仅是共振峰的位置，还得提取它们转变的过程。

所以我们提取的是频谱的包络（Spectral Envelope）。

这包络就是一条连接这些共振峰点的平滑曲线。

![img](../images/random_name/20130623210133203)



我们可以这么理解，将原始的频谱由两部分组成：包络和频谱的细节。这里用到的是对数频谱，所以单位是dB。那现在我们需要把这两部分分离开，这样我们就可以得到包络了。

为了达到这个目标，我们需要Play a Mathematical Trick。

这个Trick是什么呢？

就是对频谱做FFT。

在频谱上做傅里叶变换就相当于逆傅里叶变换Inverse FFT (IFFT)。

需要注意的一点是，我们是在频谱的对数域上面处理的，这也属于Trick的一部分。

这时候，在对数频谱上面做IFFT就相当于在一个伪频率（pseudo-frequency）坐标轴上面描述信号。

总结下，倒谱（cepstrum）就是一种信号的傅里叶变换经对数运算后再进行傅里叶反变换得到的谱。它的计算过程如下：

![img](../images/random_name/20130623211249640)



好了，到这里，我们先看看我们刚才做了什么？

给我们一段语音，我们可以得到了它的频谱包络（连接所有共振峰值点的平滑曲线）了。

但是，对于人类听觉感知的实验表明，人类听觉的感知只聚焦在某些特定的区域，

而不是整个频谱包络。



而Mel频率分析就是**基于人类听觉感知实验的。**

实验观测发现人耳就像一个滤波器组一样，

它只关注某些特定的频率分量（人的听觉对频率是有选择性的）。

也就说，它只让某些频率的信号通过，

而压根就直接无视它不想感知的某些频率信号。

但是这些滤波器在频率坐标轴上却不是统一分布的，

在低频区域有很多的滤波器，他们分布比较密集，

但在高频区域，滤波器的数目就变得比较少，分布很稀疏。



人的听觉系统是一个特殊的非线性系统，

它响应不同频率信号的灵敏度是不同的。

在语音特征的提取上，人类听觉系统做得非常好，

它不仅能提取出语义信息, 而且能提取出说话人的个人特征，

这些都是现有的语音识别系统所望尘莫及的。

如果在语音识别系统中能模拟人类听觉感知处理特点，就有可能提高语音的识别率。



### 参考资料

https://blog.csdn.net/qq_30229253/article/details/80824555

## 色度频谱

它是音乐音频的一种强大表示，其中整个频谱被投影到12个区间，代表音乐八度音的12个不同的半音（或色度）

# librosa计算上面的特征

上面提到的这些特征值都可以通过librosa来求得。





# 音频分类baseline

题目是在这里

https://www.datafountain.cn/competitions/486/datasets

先把数据集分析一下。

数据集涵盖5类不同音频，该类数据集广泛应用于音频分类的业务场景

数据文件夹包含6个文件，依次为：

|     文件类别     |     文件名     |          文件内容           |
| :--------------: | :------------: | :-------------------------: |
| 训练集音频文件夹 |     train      |     训练数据集音频文件      |
| 测试集音频文件夹 |      test      |     测试数据集音频文件      |
|     字段说明     | 字段说明.xlsx  | 训练集/测试集字段的具体说明 |
|     提交样例     | submission.csv | 仅有两个字段file_name\label |

训练集有1.2G。下载数据需要先报名。不过报名没有任何限制，点一下按钮就可以报名了。

这里是datafountain给的baseline。

https://discussion.datafountain.cn/questions/3294/answers/24436

训练集有57886条，测试集有6835条，每条数据都是一段1秒左右的语音，其中包含一个单词，一共有30种可能，因此是一个30分类任务。



特征部分采用1x32x32的**对数梅尔频谱图（Log-Melspectrogram）**，关于它的介绍网上已有很多资料。这是一种很常规的特征，几乎所有音频任务都有用到。

nn类方法通常不用MFCC，因为MFCC是频谱图经有损变换得到的，去除了相关性。

而nn具有强大的端到端学习能力，直接使用原始频谱图能提取出更多信息。（事实上，直接喂16000维的原始波形也能达到90+准确率）

https://blog.csdn.net/wherewegogo/article/details/109851123



还是借助colab来做实验。

本地训练实在是太慢了。

```
!wget http://datafountain.int-yt.com/Files/BDCI2020/486AudioClassfication/train.zip -O /content/drive/MyDrive/train.zip
```

这样来下载放到google drive上。这样就不怕colab回收导致文件丢失了。

速度一般。等等吧。

这样解压到指定目录下。

```
!unzip -d /content/drive/MyDrive/audio_classify/train /content/drive/MyDrive/audio_classify/train.zip

!unzip -d /content/drive/MyDrive/audio_classify/test /content/drive/MyDrive/audio_classify/test.zip
```

进行训练看看。

也挺慢的，不知道慢在哪里。

跑不完的。还是只能在本地的电脑上跑一下。

本地跑的话，大概500ms一张图片。

58000张图片。需要8个小时才能跑完。

# LGB多分类

https://www.jianshu.com/p/b2c95f13a9ae

# 五折交叉验证

五折交叉验证： 

把数据平均分成5等份，每次实验拿一份做测试，其余用做训练。

实验5次求平均值。

如上图，第一次实验拿第一份做测试集，其余作为训练集。

第二次实验拿第二份做测试集，其余做训练集。依此类推~

https://blog.csdn.net/u014264373/article/details/116241388

# CNN方案

上面已经用sklearn来做了baseline的参考。

现在我们看看cnn的方式，有多大的改进。

CNN是卷积神经网络，入门可以参考网上的各种介绍或者讲深度学习的书籍，

它之所以能够处理音频特征是因为**音频的特征本质上是多维时序的**，

CNN可以通过卷积来提取有效的特征，

这样就比我们手工构造特征要有效的多了，

本文使用tensorflow的keras构造了一个很简单的CNN（其实是五个，五折交叉验证），最后一层用softmax全连接层进行分类。

**网络的结构和参数有很大的调整空间**，这里交给大家自由发挥。

cnn处理的数据输入是经过librosa处理过的数据。



参考资料

https://blog.csdn.net/wherewegogo/article/details/110369729



最近在图像生成建模方面的突破取决于高质量和大规模数据集的可用性，例如 MNIST、CIFAR 和 ImageNet。

我们认识到需要一个与图像域中的音频数据集一样平易近人的音频数据集。



NSynth 是一个音频数据集，谷歌提供的。

包含 305,979 个音符，

每个音符都有独特的音高、音色和包络。

对于来自商业样本库的 1,006 种乐器，

我们通过覆盖标准 MIDI 钢琴 (21-108) 的每个音高以及五种不同的速度（25、50、 75、100、127）。

音符在前三秒保持不变，并在最后一秒衰减。



这个相当于音频版本的mnist数据集。

https://github.com/Jakobovski/free-spoken-digit-dataset/

https://www.kaggle.com/aakashns/audio-classification-using-fastai





https://magenta.tensorflow.org/datasets/nsynth



# 参考资料

1、

https://blog.csdn.net/wherewegogo/article/details/110369729

2、2020CCFBDCI训练赛之通用音频分类baseline

https://blog.csdn.net/wherewegogo/article/details/109851123