---
title: npu之深度学习要调哪些参数
date: 2021-09-28 19:20:33
tags:
	- npu

---

--

总是听说深度学习工程师自嘲为调参侠，那么深度学习的训练过程中，需要调整哪些参数呢？

我看使用keras代码的话，也没有几个参数需要调整的。

需要用到调参技巧的参数都是超参数

因此，这个问题还可以换成更专业一点：神经网络中有哪些超参数？**主要从两个方面来看**：

- 和网络设计相关的参数：神经网络的网络层数、不同层的类别和搭建顺序、隐藏层神经元的参数设置、LOSS层的选择、正则化参数
- 和训练过程相关的参数：网络权重初始化方法、学习率使用策略、迭代次数、小批量数据 minibatch的大小、输入数据相关



通常网络训练的结果一般表现为以下五种情况：

过拟合、欠拟合、恰好拟合，趋于收敛但一直在震荡以及完全不收敛。



**恰好拟合**：

从LOSS曲线上看，训练集和测试集的LOSS都已经收敛，且很接近！

模型能够拟合训练样本和测试样本的分布，且是一致的！

这样的优点就是模型的泛化能力强，简单来讲就是在训练集和测试集上效果都很好。通常表现如A图所示。

**这个时候还需要调参？？效果都这么好。需要调参！** 

主要集中在网络结构设计方面的参数，在工程项目上，同样的效果。

我们需要考虑更小、更轻量型的网络结构，计算量=====功耗，网络大小=======内存！！！

一定要学会减少计算量、较小网络大小，当然如果说你的算力随便用，后面的内容可以忽略了。

这时候可以考虑：

- 减少网络层数
- 减少不同的层的参数，主要是卷积核的数量
- 考虑深度可分离这样的轻量型卷积结构



**欠拟合**：

从LOSS曲线上看，训练集和测试集从趋势上还没有收敛！

如图B所示。（不要告诉我不知道什么是收敛，好吧..就是loss曲线变平了，如A所示）

当然，欠拟合也有可能表现在训练集和测试集上效果都一般，在训练集上的精度也不高。

这个时候，怎么办？需要调整哪些参数？

通常考虑以下几个方面的参数调整：

- 加大训练迭代次数，有可能是网络还没训练完！！这种低级错误千万不能犯
- 加大迭代次数的同时可以考虑进一步衰减调小学习率
- 添加更多的层，也有可能是网络容量不够！
- 去掉正则化约束的部分，l1\l2正则（正则主要是为了防止过拟合）
- 加入BN层加快收敛速度
- 增加网络的非线性度（ReLu），
- 优化数据集，进行数据清洗，



**过拟合**：

从样本曲线上看，从看都趋向收敛，但是测试集上的LOSS很高，甚至出现回升的现象。

如D图所示。说明模型的泛化能力很差，有可能训练集和测试集数据分布不一致，**更加可能的是模型太复杂。**

- 增加样本数量，训练样本太少或者说小样本问题，很容易导致过拟合。推荐阅读博主的另一篇文章，关于小样本问题的博客：《炼丹笔记之小样本学习》，链接地址：[https://mp.weixin.qq.com/s/6hzGMOMrG2w-54c4zyRj5g](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/6hzGMOMrG2w-54c4zyRj5g)
- 数据增强，可以参考博主的另一篇博客《炼丹笔记之数据增强》[https://mp.weixin.qq.com/s/KfiggFTzDRMjQWzvFd_C_g](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/KfiggFTzDRMjQWzvFd_C_g)
- 早停法（Early stopping），从LOSS不在下降的地方拿到模型，作为训练好的模型
- 增加网络的稀疏度，
- 降低网络的复杂度（深度）
- L1 regularization,
- L2 regulariztion,
- 添加Dropout，
- 适当降低Learning rate，
- 适当减少epoch的次数，



参考资料

1、

https://zhuanlan.zhihu.com/p/56745640

