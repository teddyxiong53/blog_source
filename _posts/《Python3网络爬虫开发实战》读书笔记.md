---
title: 《Python3网络爬虫开发实战》读书笔记
date: 2019-10-16 09:23:54
tags:
	- python

---

1

```
爬虫技术仍然是支撑一些信息融合应用（如今日头条）的关键技术。

上述内容其实都对应各自的 URL，是基于 HTTP 或 HTTPS 协议的，只要是这种数据，爬虫都可以抓取。
有时候，我们在用 urllib 或 requests 抓取网页时，得到的源代码实际和浏览器中看到的不一样。
现在网页越来越多地采用 Ajax、前端模块化工具来构建，整个网页可能都是由 JavaScript 渲染出来的，也就是说原始的 HTML 代码就是一个空壳。
body 节点里面只有一个 id 为 container 的节点，但是需要注意在 body 节点后引入了 app.js，它便负责整个网站的渲染。
但是在用 urllib 或 requests 等库请求当前页面时，我们得到的只是这个 HTML 代码，它不会帮助我们去继续加载这个 JavaScript 文件，这样也就看不到浏览器中的内容了。
对于这样的情况，我们可以分析其后台 Ajax 接口，也可使用 Selenium、Splash 这样的库来实现模拟 JavaScript 渲染。
那么，这种神秘的凭证到底是什么呢？其实它就是会话和 Cookies 共同产生的结果
在了解会话和 Cookies 之前，我们还需要了解 HTTP 的一个特点，叫作无状态。
HTTP 的无状态是指 HTTP 协议对事务处理是没有记忆能力的，也就是说服务器不知道客户端是什么状态。

这时两个用于保持 HTTP 连接状态的技术就出现了，它们分别是会话和 Cookies。
我们可以理解为 Cookies 里面保存了登录的凭证，有了它，只需要在下次请求携带 Cookies 发送请求而不必重新输入用户名、密码等信息重新登录了。

因此在爬虫中，有时候处理需要登录才能访问的页面时，我们一般会直接将登录成功后获取的 Cookies 放在请求头里面直接请求，而不必重新模拟登录。

会话，其本来的含义是指有始有终的一系列动作 / 消息。比如，打电话时，从拿起电话拨号到挂断电话这中间的一系列过程可以称为一个会话。
Cookies 携带了会话 ID 信息，服务器检查该 Cookies 即可找到对应的会话是什么，然后再判断会话来以此来辨认用户状态。

而且恰恰是由于关闭浏览器不会导致会话被删除，这就需要服务器为会话设置一个失效时间，当距离客户端上一次使用会话的时间超过这个失效时间时，服务器就可以认为客户端已经停止了活动，才会把会话删除以节省存储空间。

在 Python 2 中，有 urllib 和 urllib2 两个库来实现请求的发送。而在 Python 3 中，已经不存在 urllib2 这个库了，统一为 urllib，
了解一下 urllib 库，它是 Python 内置的 HTTP 请求库，也就是说不需要额外安装即可使用。
有4个模块：
1、request。
2、error。
3、parse。
4、robotparser。不常用。
我们了解了 urllib 的基本用法，但是其中确实有不方便的地方，比如处理网页验证和 Cookies 时，需要写 Opener 和 Handler 来处理。为了更加方便地实现这些操作，就有了更为强大的库 requests，有了它，Cookies、登录验证、代理设置等操作都不是事儿。

这里我们加入了 headers 信息，其中包含了 User-Agent 字段信息，也就是浏览器标识信息。如果不加这个，知乎会禁止抓取。

所以，利用 Session，可以做到模拟同一个会话而不用担心 Cookies 的问题。它通常用于模拟登录成功之后再进行下一步的操作。

Session 在平常用得非常广泛，可以用于模拟在一个浏览器中打开同一站点的不同页面，后面会有专门的章节来讲解这部分内容。

此外，requests 还提供了证书验证的功能。当发送 HTTP 请求的时候，它会检查 SSL 证书，我们可以使用 verify 参数控制是否检查此证书。其实如果不加 verify 参数的话，默认是 True，会自动验证。

对于某些网站，在测试的时候请求几次，能正常获取内容。但是一旦开始大规模爬取，对于大规模且频繁的请求，网站可能会弹出验证码，或者跳转到登录认证页面，更甚者可能会直接封禁客户端的 IP，导致一定时间段内无法访问。

若代理需要使用 HTTP Basic Auth，可以使用类似 http://user:password@host:port 这样的语法来设置代理，示例如下：
proxies = {'https': 'http://user:password@10.10.1.10:3128/',}

除了基本的 HTTP 代理外，requests 还支持 SOCKS 协议的代理。
pip3 install "requests[socks]"
实际上，请求分为两个阶段，即连接（connect）和读取（read）。
上面设置的 timeout 将用作连接和读取这二者的 timeout 总和。
可以这样分开指定。
timeout=(5, 30)

此时可以使用 requests 自带的身份认证功能，示例如下：
from requests.auth import HTTPBasicAuth  
r = requests.get('http://localhost:5000', auth=HTTPBasicAuth('username', 'password')) 
当然，如果参数都传一个 HTTPBasicAuth 类，就显得有点烦琐了，所以 requests 提供了一个更简单的写法，可以直接传一个元组，它会默认使用 HTTPBasicAuth 这个类来认证。
r = requests.get('http://localhost:5000', auth=('username', 'password'))
此外，requests 还提供了其他认证方式，如 OAuth 认证，不过此时需要安装 oauth 包，
pip3 install requests_oauthlib

刚才我们写的正则表达式其实比较复杂，出现空白字符我们就写 \s 匹配，出现数字我们就用 \d 匹配，这样的工作量非常大。其实完全没必要这么做，因为还有一个万能匹配可以用，那就是.（点星）。其中.（点）可以匹配任意字符（除换行符），（星）代表匹配前面的字符无限次，所以它们组合在一起就可以匹配任意字符了。有了它，我们就不用挨个字符地匹配了。

使用上面的通用匹配 .* 时，可能有时候匹配到的并不是我们想要的结果。

match 方法是从字符串的开头开始匹配的，一旦开头不匹配，那么整个匹配就失败了。

这里就有另外一个方法 search，它在匹配时会扫描整个字符串，然后返回第一个成功匹配的结果。
因此，为了匹配方便，我们可以尽量使用 search 方法。

这里需要提取的部分用小括号括起来，以便用 group 方法提取出来，

search是返回第一个匹配的，findall是返回所有符合的。
sub函数，是用来修改文本内容的。
在实际应用中，用sub来去掉比较麻烦会影响我们但是又不需要的部分内容。会简化我们的工作。
```



使用requests和正则表达式来提取猫眼电源的前100名的数据。

对应的url是：https://maoyan.com/board/4

一共分为10页，每页10条。

第二页的网址是：https://maoyao.com/board/4?offset=10

很容易发现网址的规律。

第一步，先把网页获取下来。



```
由于目标计算机积极拒绝，无法连接。
```

猫眼现在好像不让爬了。

不是不让爬，是我的headers里的User-Agent设置不对。

用chrome浏览器打开目标页面，按F12打开调试窗口，观察到每一个条目，都是在一个dd标签内部。

需要提取的内容有：

```
排名
	排名是在i标签里。
	<dd>.*?board-index.*?>(.*?)</i>
图片
	
名称
	
主演
发布时间
评分
```

我们可以用一个正则表达式一次性把这些信息都提取出来。



参考资料

1、

https://python3webspider.cuiqingcai.com/