---
title: 《Linux多线程服务端编程》读书笔记
date: 2019-02-22 14:5-:51
tags:
	- cpp
---



muduo网络库学习

# 1

析构的技巧
对象的析构是同步的。
当最后一个指向对象x的shared_ptr离开它的作用域的时候，x会在同一个线程内被析构。
这个线程不一定是对象x诞生的线程。
这个特性是一把双刃剑：
如果对象x的析构非常耗时，它有可能会阻塞我们的关键线程。
但是，我们也可以利用这个特性，专门用一个线程来做析构操作。

RAII（资源获取即初始化）
这个可以说是c++区别于其他语言的一个最重要的特性。
不懂得RAII的c++程序员不是一个合格的c++程序员。
初学c++的教条是：
new和delete要匹配，new了之后要记得delete。
如果使用RAII，那么教条就要被改为：
每一个明确的资源配置动作（例如new）都应该在单一语句中执行，
并在该语句中立刻将配置获得的资源交给handle对象（例如shared_ptr），
程序中一般不初学delete。
shared_ptr避免循环引用的一般做法：
owner持有执行child的shared_ptr。child持有执行owner的weak_ptr。

shared_ptr有个特性，可以指定析构时调用的函数。
## 1.11

为了使用shared_from_this()，StockFactory不能是stack object，
而必须是heap object。并且由shared_ptr来管理它的生命周期。

有时候，我们需要这样的一个功能：
如果对象还活着，就调用它的成员函数，否则就忽略。

用weak_ptr就可以实现这个。
我们可以把weak_ptr绑到std::function里。
这样对象的生命周期就不会被延长。
然后在回调的时候，先尝试把weak_ptr提升为shared_ptr，
如果提升成功，就说明接受回调的对象还存在，可以正常执行回调。
如果提升失败，就忽略。

通常Factory对象是一个singleton，在程序运行期间不会销毁。
这里只是为了展示弱回调技术，这个技术在事件通知里非常有用。

## 1.12 替代方案
除了shared_ptr和weak_ptr的方案，要做到c++里线程安全的对象回调和析构。
还有下面的方法。
1、用一个facade模式（外观模式）。但是这个会导致串行，效率不高。
2、只创建不销毁。这个是无奈之举。
3、使用unique_ptr。能避免引用计数的开销，某些场景可以替换shared_ptr。

其他的语言是怎么处理的？
有垃圾回收就很好办。
在c和c++里，因为指针的存在，实现全自动垃圾回收很困难。

## 1.13 小结
对于多线程编程，要花专门的时间系统学习，严禁半桶水上阵。

分析可能出现的race condition，不仅是多线程编程的基本功，也是设计分布式系统的基本功。
需要反复历练，形成一定的思考范式。
并积累一些经验教训，才能少犯错误。
在单cpu时代的多线程编程经验，到了多cpu时代不一定有效。
因为多cpu能做到真正的并行，每个cpu看到的事件发生顺序不一定完全相同。
就如同狭义相对论里，每个观察者都有自己的时钟。
在不违反因果律的前提下，可能发生十分违反直觉的事情。

尽管上面的内容都是在讨论如何安全地使用（包括析构）跨线程的对象。
但是建议尽量减少跨线程的对象。

## 1.14 observer之谬
observer是一个广泛使用的设计模式。
但是它有本质的问题。
observer的本质问题在于它的面向对象的设计。
observer是基类，这个带来了非常强的耦合。
这个耦合仅次于友元。
这种耦合不仅限制了成员函数的名字、参数、返回值，
还限制了成员函数所属的类型。（必须是observer的子类）

observer类是基类，这意味着，如果Foo想要观察两个类型的事件（例如时间和温度），
那么就要使用多继承。这个还不是最糟糕的。
如果要重复观察同一类型的时间两次（例如1秒一次的心跳和30秒一次的自检）。
就需要使用一些特别的技巧。
因为不能从一个基类继承两次。

现在的语言一般可以绕过observer模式的限制。
java里，使用匿名内部类。也可以用闭包。
c++用function和bind。

# 2. 线程同步精要
并发编程有两种模型：
1、message passing。
2、shared memory。
对于分布式，只有message passing这一种。

线程同步的四大原则：
1、尽量减少同步的内容。
2、使用高级的并发编程组件。例如TaskQueue、CountDownLatch。
3、如果必须使用底层的同步原语时，只使用非递归的mutex和condition。
4、除了使用atomic整数外，不要自己编写lock-free代码。

## 2.1 mutex
这个应该是被用得最多的同步原语了。
简单来说，它保护了临界区。
我们用它，主要是用例保护共享数据。

使用原则是：
1、用RAII手法封装mutex的create、destroy、lock、unlock这4个操作。
2、只使用非递归的mutex。
3、不手动调用lock和unlock函数。
4、在每次构造Guard对象的时候，思考一路上已经持有的锁。
防止因为加锁顺序不同而导致死锁。
由于Guard是栈上对象，所以看函数调用栈就可以看出锁的情况。很方便。

次要原则是：
1、不使用跨进程的mutex。进程间通信只用tcp socket。
2、必要的时候，可卡因使用PTHREAD_MUTEX_ERRORCHECK来查错。

mutex也是最简单的同步原语，按照上面的原则，几乎不可能用错。
即使出错，有可以很快定位到。

为什么只使用非递归的mutex？
递归的带来问题不容易排除，而收益不大。
而非递归的，可以把问题暴露地很明显。就是死锁。

如果一个函数，既可能在已经加锁的情况下使用，也可能在没有加锁的情况下使用。
那么就拆开成2个函数：
1、func。里面加锁调用第二个函数。
2、funcWithLockHold。不加锁。把之前的函数内容放这里。

## 2.2 condition
mutex只是加锁原语，不是等待原语。

condition只有一种用法，不可能用错。
对于wait端：
1、必须与mutex一起用，表示condition受到mutex保护。
2、在mutex已经上锁的时候，才能调用wait。
3、把判断bool调节和wait放到while循环里。

写成代码是这样：
```
muduo::MutexLock mutex;
muduo::Condition cond(mutex);
std::deque<int> queue;
int dequeue()
{
    MutexLockGuard lock(mutex);
    while(queue.empty()) {
        cond.wait();//这一步会自动unlock，不会与enqueue操作死锁。
    }
    int top = queue.front();
    queue.pop_front();
    return top;
}
```
上面要用while里调用wait，而不是if。
这个是因为spurious wakeup问题。这个是多线程面试中经常问到的点。

对于signal和broadcast端：
1、不一定要在mutex已经上锁的情况下调用signal。
2、在signal之前，一般要修改bool表达式。
3、修改bool表达式通常需要mutex保护。
4、注意区分signal和broadcast。
	broadcast通常用于表明状态变化。
	signal用于表示资源可用。

写成代码是这样：
```
void enqueue(int x)
{
    MutexLockGuard lock(mutex);
    queue.push_back(x);
    cond.notify();//这个可以不受mutex保护。
}
```
上面的dequeue和enqueue实际上实现了一个简单的容量无限的BlockingQueue。

mutex和condition都是底层的同步原语，很少直接使用。
一般用它来实现高层的BlockingQueue和CountDownLatch。

CountDownLatch是一种易用且常用的同步手段，主要有两种用途：
1、主线程发起多个子线程，等这些子线程都执行完之后，主线程才继续执行。
2、主线程发起多个子线程，子线程都等待主线程，主线程在完成一些其他任务后，
	通知所有的子线程开始执行。相当于发令枪。

当然我们可以用condition来完成这个操作。
但是用CountDownLatch逻辑上看起来更加清晰。

## 2.3 不要用rwlock和semaphore

rwlock看上去很美，因为它区分了读和写的处理。

## 2.4 封装MutexLock、MutexLockGuard、Condition

这几个类都不允许拷贝构造和赋值。

## 2.5 线程安全的singleton实现

研究singleton的线程安全实现的历史，会发现很多有意思的事情。
人们一度认为double checked locking（简称DCL）是王道。兼顾了效率和正确性。
后面有大牛指出由于乱序执行的影响，DCL是靠不住的。
对于java来说，可以通过内部静态类的装载来实现。
c++就麻烦多了。有这些处理手段：
1、要么每次都锁。
2、要么eager initialize。
3、要么用memory barrier。

后面java5修改了内存模型，并给volatile赋予了acquire/release的语义。
这样在java里，DCL又是安全的了。

但是c++的内存模型还在修订中。

但是，实际上可以有简单的解决办法，就是用pthread_once来做。
```
template<typename T>
class Singleton
{
public:
    static T& instance() {
        pthread_once(&ponce_, &Singleton::init);
        return *value_;
    }
private:
    Singleton();
    ~Singleton();
    static void init() {
        value_ = new T();
    }
private:
    static pthread_once_t ponce_;
    static T* value_;
};
template<typename T>
pthread_once_t Singleton<T>::ponce_ = PTHREAD_ONCE_INIT;
template<typename T>
T* Singleton<T>::value_ = NULL;
```
上面这个Singleton没有任何花哨的技巧。
它用pthread_once_t来保证lazy initialize的线程安全。
线程安全性由pthread库来保证。

## 2.6 sleep不是同步原语
sleep函数只能出现在测试代码里，用来帮助复现一些死锁问题。
生产代码里，线程的等待可以分为两种：
1、等待资源可用，阻塞在select或者condtion上。
2、等待进行临界区，阻塞在mutex上。这种等待时间要很短。

在程序的正常执行中，如果需要等待一段已知的时间，就应该往event loop里注册一个timer。
在timer的回调里做这个事情。
线程是宝贵的资源，不要轻易阻塞。

## 2.7 总结

## 2.8 借助shared_ptr实现copy-on-write

# 3 多线程服务器程序编程模型

网络应用程序，可以归纳为“收到数据，算一算，把结果发出去”。

## 3.1 进程与线程

可以把进程比喻为人。
这个比喻为我们进行思考提供了一个框架。

每个人都有自己的记忆（memory），人与人直接可以通过谈话（消息传递）来交流。
谈话既可以是面谈（同一台服务器），也可以在电话里谈（不同服务器，通过网络通信）。
面谈和电话谈的主要区别在于：
面谈可以立刻知道对方是否已经离开，而电话谈只能通过周期性的询问“你还在不在”来判断。

基于这个比喻，我们可以把其他的概念也对应起来：
容错：有人离开了怎么办？
扩容：新人加入。
负载均衡：A的活给B干。

线程的概念是从1993年开始流行起来的。
到现在也不长。
比unix的历史短了不少。
线程的出现给unix填了不少的乱子。
很多的C库函数都不是线程安全的。
需要重新定义。
signal的语意也复杂化了。
最早支持多线程的是Solaris和windows NT。都是1993年发布的。
1995年，posix thread标准确定。

线程的特点是共享地址空间。这样就可以高效地共享数据。

多线程的价值，主要是为了更好地发挥多核的性能。
在单核时代，多线程没有什么意义。

## 3.2 单线程服务器的常用编程模型

对于这个话题Unix network programming这本书在第6章和第30章做了很好的总结。
在高性能的网络程序里，使用最广泛的的，就是：
非阻塞io + io多路复用这种模型。
这种模型也叫Reactor模型。

这些软件都是采用了这种模型的。
1、lighttpd。
	单线程。
2、libevent。

在Reactor模型里，程序的基本结构是：
一个eventloop，以事件驱动和事件回调的方式实现业务逻辑。

Reactor模型的优点很明显：
1、编程简单。
2、效率不错。
对于io密集型的应用是很好的选择。

Reactor模型的缺点：
1、回调函数必须非阻塞。
2、割裂业务逻辑，分散到多个回调函数中。

python等现代编程语言，就用协程来应对。

## 3.3 多线程服务器的常用编程模型

有这些：
1、每个请求创建一个线程。阻塞io。
2、使用线程池。阻塞io。
3、非阻塞io+ io多路复用。
4、Leader/Follower等高级模式。

我们用第三种。
具体说是，one loop per thread。
每个io线程都有一个event loop（或者叫Reactor）。
用于处理读写和定时事件。

这种方式的好处是：
1、线程数量是固定的，不会频繁创建和销毁。
2、可以很方便地在线程间调配负载。
3、io事件发生的线程是固定的。

对于没有io而光有计算任务的线程，使用event loop有点浪费。
可以使用TaskQueue的补充方案。
封装闭包，往里面扔就是了。

## 3.4 进程间通信只用tcp

使用tcp的好处有：
1、双向的。
2、tcp端口由一个进程独占，而且os会自动回收，即使意外退出，也不会留下垃圾。
3、两个进程通过tcp通信，一个崩溃了，os会关闭连接，另外一个可以很快感知到。
4、tcp天然可记录，可重新，用tcpdump可以记录下通信内容，方便分析。还可以用tcpcopy来做压力测试。
5、还能跨语言使用。

使用tcp长连接的好处：
1、容易定位分布式系统里的服务之间的依赖关系。
2、通过接收和发送队列的长度也容易定位网络或程序故障。

## 3.5 多线程服务器的适用场合

必须使用单线程的场合：
1、程序里使用了fork。
2、需要限制程序的cpu占用率。

只有单线程的程序能进行fork。
多线程fork后有很多的问题。

一个程序fork后，一般有两种行为：
1、马上执行exec，变成另外一个程序。例如shell和inetd。
2、不调用exec，继续运行当前程序。

单程序程序的优点：
1、从编程的角度，简单。

单线程程序的缺点：
1、它是非抢占的。

## 3.6 多线程服务器的适用场合答疑


# 4 c++多线程系统编程精要

## 4.1 基本线程原语的选用

pthread的接口有100多个。常用的不过10个左右。
分别如下：
1、线程的create和join。
2、muetx的create、destroy、lock、unlock。
3、condition的create、destroy、wait、notify、broadcast。

## 4.2 C/C++系统库的线程安全性

老的c/c++标准没有涉及到线程。
C11/C++11规定了程序在多线程下的语意。

对于标准而言，关键的不是定义线程库。
而是规定内存模型。
特别是规定一个线程对某个共享变量的修改，什么时候可卡因被其他线程看到。
这个叫做内存序（memory ordering）或内存能见度（memory visibility）。
从理论上说，如果没有正确的内存模型，编写正确的多线程程序属于碰运气的行为。

新标准的意义在于让编写跨平台的多线程程序更有保障了。

线程的出现给系统函数库代理冲击，破坏了unix出现20年来一贯的编程传统和假定。
1、errno不再是一个全局变量。
2、有些纯函数不受影响。
3、有些影响全局状态或者有副作用的函数可以通过加锁来实现线程安全，例如malloc和free。printf。
4、有些返回或者使用静态空间的函数不可能做到线程安全，因此要另外提供线程安全的版本。
	例如ctime_r、stderror_r。
5、传统的fork并发模型不再适用于多线程程序。

不过，os支持多线程已经有20多年了。早期的一些性能方面的缺陷都基本被弥补了。
我们不必担心系统调用的线程安全性，因为系统调用对于用户态的程序来说，是原子的。

posix给我们列出了一份黑名单，里面是线程不安全的函数。

现在glibc里的大部分函数都是线程安全的。FILE*系列的都是线程安全的。
有些函数，提供了非线程安全的版本，用来满足极致性能要求。

尽管单个函数是线程安全的，但是多个函数放在一起的时候就不一定了。

c++标准库里的绝大多数泛型算法是线程安全的。
因为这些都是无状态的纯函数。

c++的iostream不是线程安全的。
所以我们尽量用printf来替换cout。

## 4.3 Linux上的线程id

pthread提供了pthread_self函数来返回当前线程的标识。
类型是pthread_t。
pthread_t不一定是一个数值类型，可能是一个结构体。
因此pthread还提供了一个pthread_equal来判断2个id是否相等。

这个带来了一系列的问题：
1、无法打印输出pthread_t。因为不知道它的具体类型。
2、无法比较pthread_t的大小或者计算其hash值，因此无法关联容器的key。
3、无法定义一个非法的pthread_t值。
4、pthread_t只在进程内部有效。

在glibc的实现里，pthread_t是一个结构体指针。
它指向一块内存，而内存会被反复使用，这就导致pthread_t可能重复。

综上，pthread_t不适合用来做线程的标识符。

那怎么办呢？在Linux上，建议使用gettid这个系统调用的返回值作为线程id。
这样做的好处有：
1、它的类型的pid_t，通常是一个较小的整数，方便在日志里输出。
2、在现代Linux里，它直接表示内核的任务调度id，因此在/proc文件系统里可以找到对应的项目。
	/proc/pid/task/tid
3、在其他系统工具中也很容易定位到某一个线程。例如top工具。
4、任何时刻都是全局唯一的。
5、0是非法值。

但是glibc没有封装这个系统调用，所以我们要自己实现。
封装gettid也很简单，但是每次执行都执行一次系统调用似乎没有必要。
我们可以把结果缓存起来。

## 4.4 线程create和destroy的守则

create比destroy要简单得多。
create需要遵循的原则：
1、程序库不应该在未提前告知的情况下创建自己的background线程。
2、尽量用相同的方式创建线程。
	这样是方便登记管理。
3、进入main函数之前，不要启动线程。
	因为这样会影响全局对象的安全构造。
	c++保证在进入main之前完全全局对象的构造。
4、线程的创建最好在初始化阶段完成。

线程的销毁方式有：
1、自然死亡。
	从线程处理函数返回。
2、非正常死亡。
	段错误等。
3、自杀。
	pthread_exit。
4、他杀。
	pthread_cancel。
	
正常的退出方式只有自然死亡这一种。
cancel point这种东西没有实用价值。

## 4.5 善用`__thread`关键字
__thread是gcc内置的线程局部存储设施。
比pthread_key_t效率高很多。
跟全局变量的效率类似。

__thread的实用限制：
1、只能修饰POD类型。
2、不能修饰class类型。
	因为无法自动调用构造函数和析构函数。
3、不能修饰函数的局部变量和类的成员变量。
4、修饰的变量初始化只能用编译器常量。

使用__thread的情况：
1、不值得作为全局变量保护起来的。

## 4.6 多线程与IO

在进行多线程网络编程的时候，几个自然的问题是：
1、如何处理io？
2、能否多个线程同时读写同一个socket fd？
3、多个线程处理同一个socket，效率会提高吗？

多个线程处理同一个socket，很麻烦，得不偿失。
socket读写的特点是不保证完整性。
读100字节，可能只返回20字节。
写操作也是如此。

如果两个线程同时read一个socket，两个线程几乎同时各自收到一部分数据，如何把数据拼成
完整的消息？如何知道哪部分数据先到达？

为了简单起见，我认为多线程程序应该遵循的原则是：
1、每个fd只由一个线程操作，从而轻松解决消息收发的顺序性问题。
2、epoll也遵循相同的原则，把对epoll fd的操作，都放到同一个线程里进行。

## 4.7 用RAII包装fd

现代c++的一个特点是对象生命周期管理的进步，体现在不需要手动delete对象。
在网络编程里，有的对象是长命的（如TCPServer）。有的对象是短命的（如TCPConnection）。
长命的对象一般和整个程序一样长。
一般可以：
1、使用全局对象。
2、做成main的栈上对象。

对于短命的对象，其生命周期不一定完全由我们控制。
例如，某个客户端断开了tcp连接，服务端的对应的TCPConnection也就要结束生命周期了。
但是我们不能马上delete这个对象。
因为其他地方可能还持有它的引用。
贸然delete会造成空悬指针。

所以用shared_ptr来管理TcpConnection是很好的选择。

## 4.8 RAII和fork
fork会破坏RAII。

## 4.9 多线程和fork
多线程和fork的协作性很差。
这个是posix的历史包袱。

## 4.10 多线程与signal
signal和多线程是水火不容。
不要使用signal。

## 4.11 Linux新增系统调用的启示

大致从Linux内核2.6.27开始，凡是会创建fd的syscall都增加了一个额外的flags参数。
用来指定O_NONBLOCK和FD_CLOEXEC。
包括：
accept4
eventfd2
inotify_init1
pipe2
signalfd4
timerfd_create

这个反映了Linux服务端开发的一个方向，就是非阻塞io+io多路复用。
直接在创建fd的时候，就设置为非阻塞，节省一次系统调用。

而FD_CLOEXEC，则反映了现在fork后，马上执行新程序。不要跟父进程有什么瓜葛。

# 5 高效的多进程日志

对于关键进程，一般需要记录：
1、每条内部消息的id。
2、收到的每条外部消息的全文。
3、发出的每条消息的全文。
4、关键内部状态的变更。

每条记录都有时间戳，这样就可以追踪一个事件的来龙去脉。

日志主要是给运维人员看的。所以要清晰明确。

一个日志库，分为前端和后端2个部分。

日志输出风格有两种，一种printf风格，一种stream风格。

## 5.1 功能需求
日志消息：
1、级别控制。
2、目的地控制，可以是文件、socket、smtp等。
3、格式可配置。
4、运行时过滤设置。

我认为只有第一项是必须的，其余都可以不要。
目的地，只选择本地文件。
既然是以本地文件为目的地，那么日志的滚动是必须的。
这样可卡因简化日志归档的实现。
rolling的调节通常有2个：文件大小和时间。

一个典型的日志文件的文件名如下：
logfile_test.20191122-144022.192.168.56.101.3605.log
process_name.time.host.pid.log

muduo的日志文件滚动没有采用文件改名的办法。
而是这样风格：
xx.log
xx.log.1
xx.log.2.gz
这样的好处是：
1、xx.log始终是最新的日志，方便编写某些及时解析日志的脚本。

往文件里写日志的一个常见问题是，万一程序崩溃，那么最后几条日志往往就丢失了。
因为日志库不能每条消息都flush硬盘。
这样性能开销太大。

muduo的日志采用了两个办法来解决这个问题
1、定期（每隔3s）flush一次。
2、每条内存里的日志都带有一个cookie，它的值是某个函数的地址，这样在coredump文件里找cookie就可以找到还没有来得及写入磁盘的消息。

日志消息格式有如下几个要点：
1、尽量每条日志占一行。这样方便用awk等工具来分析。
2、时间戳精确到us。
3、时钟使用0时区的时间。
4、打印线程id。方便分析多线程程序的时序，也可以追踪死锁。
5、打印日志级别。
6、打印文件名和行号。

每一行日志的前4个字段的宽度是固定的。
用空格来分隔，便于脚本解析。
不要使用[]这样不便于正则表达式解析的字符。

## 5.2 性能需求
编写Linux服务端程序的时候，需要日志库的效率足够高。
高效率体现在：
1、每秒写几千万条日志没有明显的性能损失。
2、能应对1GB/min的日志产生场景。
3、不阻塞正常的执行流程。
4、在多线程程序中，不会乱。

为了达到这样的性能，muduo的日志库有几点优化值得一提：
1、时间戳字符串里的日期和时间是缓存的。
	在1s内的多条日志只需要格式化us部分。
2、日志消息的前面4个字段是定长的，可以避免运行时求取长度。
3、线程id是预先格式化为字符串的。
4、文件名在编译器就确定了。

## 5.3 多线程异步日志

为了避免多线程导致日志混乱的问题，异步日志是必须的。
需要一个队列将日志前端的数据传送到后端。

muduo的日志库采用的是双缓冲技术。
基本思路是：
1、准备2个buffer：A和B。
2、前端负责往A里面写数据，后端负责把B的数据写入文件。
3、当A写满了，交互A和B。

实际实现上，使用了4个缓冲区，这样就进一步减少或者避免了日志前端的等待。

## 5.4 其他方案

# 6 muduo网络库简介

## 6.1 由来
为什么需要网络库？
使用socket api进行网络编程是一件很容易上手的技术。

## 6.2 安装
muduo使用了Linux较新的系统调用，要求Linux内核版本高于2.6.28。

## 6.3 目录结构

## 6.4 使用教程
tcp网络编程的本质是处理三个半事件：
1、连接的建立。
	服务端：
		accept。
	客户端：
		connect。
	tcp连接一旦建立，对于客户端和服务端就是平等，都可以各自收发数据。
2、连接的断开。
	主动断开：
		shutdown、close
	被动断开：
		read返回0 
3、消息到达。
	这个是最重要的一个事件。
	对它的处理方式决定了网络编程的风格。
	是阻塞还是非阻塞，如何处理分包，应用层的缓冲如何设计。
4、消息发送完毕。
	这个算半个。
	对于低流量的服务，这个事件可以不关心。
	这里的发送完毕，表示把数据写入到os的缓冲器，交给协议栈去发送。
	
这里面有很多难点，也有很多细节需要注意，比如说：
1、如果要主动close连接，如何保证对方已经收到全部数据？
2、如果应用层有缓冲（这个对于非阻塞网络编程是必须的），如何保证先发送完缓冲里的数据再断开连接？
3、如果主动发起连接，但是被对方主动拒绝，如何定期带backoff地重试？
4、非阻塞网络编程，应该用边沿触发还是电平触发？
	如果是电平触发，那么什么时候关注EPOLLOUT事件？会不会造成busy-loop？
	如果是边沿触发，如何防止漏读造成的饥饿？

在非阻塞网络编程里，为什么要在应用层使用缓冲区？
1、假设应用要发送40K的数据，但是os的tcp发送缓冲区只有25K。
	那么生效的15K数据怎么办？
	如果等待os缓冲区可用，那么就会阻塞当前线程，因为不知道对方什么时候收到并读取数据。
	因此，需要应用层把15K数据缓存起来。
	

在非阻塞网络编程里，为什么需要使用应用层接收缓冲区？
1、假设一次读到的数据不够一个完整的数据包，那么就应该把这些内容放到缓冲区。
	然后等剩余的数据到来。
	
那么应该如何设计缓冲区呢？
1、一方面我们希望减少系统调用的次数，一次读的数据越多越好。
	这样看就要一个大的缓冲区。
2、另一方面，我们希望减少内存占用。
	加入有10000个连接，每个连接分配50K的缓冲区，就需要占用1G的内存。
	而大多数时候，都并不需要这么大的缓冲区。
	muduo使用readv结合栈上空间巧妙地解决了这个问题。



## 6.5 性能评测

## 6.6 详解muduo多线程模型

本节以一个sudoku solver为例，回顾了并发服务程序的多种设计方案。
并介绍了使用muduo网络库编写多线程服务器的两种最常用手法。

sudoku可以看做是echo服务器的变种。
挑战在于如何发挥现在多核硬件的能力。

我们先看一个单线程版本。

协议：
以\r\n结尾的文本行协议。使用tcp长连接。
客户端在不需要服务的时候主动断开。
请求：[id:]81位的数字。
响应：[id:]81位的数字或者NoSolution。

id是可选的。
用来区分请求的先后。

基于这个协议，我们可以用telnet来模拟客户端测试。
不需要专门写sudoku client。

sudoku是一个计算密集型的任务。
它的瓶颈是cpu。
为了让这个单线程的程序充分利用cpu资源，一个最简单的办法是在同一台服务器上部署多个
进程。
但是每个进程就要自己一个专门的端口。
对用户使用很不方便。
如果在前面再部署一台负载均衡服务器，又太小题大做了。

有没有办法，可以做到：
1、在一个端口上提供服务。
2、同时利用多核的优势。

当然有，而且不只一种。

2000年左右，第一次互联网大潮兴起，业界对高并发http服务器的需求大大推动了这一领域的研究。