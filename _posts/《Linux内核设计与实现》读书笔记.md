---
title: 《Linux内核设计与实现》读书笔记
date: 2019-11-30 16:48:31
tags:
	- Linux

---

接口定义合理，代码风格一致。
一次做一件事情，做到完美。
这是Linus倡导的内核的开发原则。

# 1 Linux内核简介

## 1.1 Unix的历史
Unix取得成功的原因：
1、unix很简洁。
	不想其他的系统，有上千个syscall。
	unix只有200多个。而且用途非常明确。
2、unix里，一切都是文件。
	这样处理接口就可以统一。
3、使用C语言编写。
	这样可移植性就很高。
4、unix创建进程很快。
	fork的优点。
5、简单高效的ipc方式。
	

## 1.2 追寻Linus足迹：Linux简介

## 1.3 操作系统和内核简介

## 1.4 Linux内核与传统Unix内核比较
Linux是宏内核，也就是说，Linux内核运行在单独的内核地址空间上。
不过，Linux吸收了微内核的优点：
1、模块化设计。
2、抢占式内核。
3、支持内核线程。
4、动态加载内核模块的能力。
Linux避免了微内核设计上性能的缺陷。
让所有的事情都运行在内核态，直接调用函数，无须消息传递。

Linux是实用主义的胜利。

## 1.5 Linux内核版本

## 1.6 Linux内核开发社区

# 2. 从内核出发

## 2.1 获取内核源代码

## 2.2 内核源码树

## 2.3 编译内核

## 2.4 内核开发的特点

1、内核编程不能使用C库函数和C库头文件。
	很多函数在内核里有一个简单的实现。
2、内核编程必须使用GNU C。
	内联函数
	内联汇编
	分支声明unlikely这种。
	
3、内核编程缺乏内存保护机制。
	内存错误会导致oops。
	
4、内核编码不要使用浮点计算。
	内核不能完美地支持浮点操作。
	
5、内核里每个进程只有一个很小的定长的栈。
	32位的是8K。64位的是16K。
	
6、因为内核支持异步中断、强制和smp，所以需要随时注意同步和并发。
7、要考虑可移植性。

# 3. 进程管理

## 3.1 进程

每个线程有一个独立的程序计数器、进程栈、一组进程寄存器。

内核调度的对象是线程，而不是进程。

Linux实现的线程其实是一种特殊的进程。

在现代os里，进程提供两种虚拟机制：
1、虚拟处理器。
2、虚拟内存。

## 3.2 进程描述符和任务结构

内核把进程的列表存放在叫task list的双向循环链表里。
链表里的每一项都是一个task_struct。
里面存放了一个进程的所有信息。
task_struct结构体比较大。
在32位机器上，大概是1.7KB。

Linux通过slab分配器来分配task_struct。
这样可以达到2个目的：
1、对象复用。
2、缓存着色。

在进程的内核栈的底部，有一个struct thread_info，里面有一个task_struct指针。

pid的存放
pid默认是32768，这个对于桌面系统够用了。
之所以是这个值，是为了跟老的系统兼容，之前是short类型的。
对于服务器是不够的。
可以通过修改/proc/sys/kernel/pid_max来修改。

内核中大部分处理进程的代码，都是通过task_struct来做的。

因此，通过current宏查找到当前正在运行的进程的pid就很主要。
根据硬件的不同，current宏的实现也不同。

进程的状态，有5种：
1、running。
2、interruptible。
	睡眠中，可以被提前唤醒。
3、uninterruptible。
	睡眠中，不能被提前唤醒。
4、traced。
	正在被追踪，例如被ptrace追踪。
5、stopped。
	收到STOP信号时。
	调试期间也是这种状态。
	
进程家族树
所有的用户进程都是pid为1的init进程的后代。

## 3.3 进程的创建

Linux通过clone这个syscall来实现fork函数。
clone有一些标志。
fork、vfork、__clone这些函数都是调用了clone，只是设置的标志不一样。

vfork
除了不拷贝父进程的页表项之外，vfork和fork功能相同。

## 3.4 线程在Linux中的实现
线程机制是现代编程技术中常用的一种抽象机制。
这种机制提供了在同一进程地址空间进行并发的能力。

从Linux内核的角度说，并不存在线程的概念。
所有的线程都是进程。


内核线程
内核经常需要在后台执行一些操作。
这种任务通过内核线程来完成。

内核线程跟普通进程的区别：
内核线程没有独立的地址空间，task_struct的mm指针为NULL。
内核进程和普通进程一样，可以被调度和抢占。

内核确实是把一些任务交给内核线程去做。
向flush和ksofirqd。
你用top可以看到不少的内核线程。

## 3.5 进程终结

当一个进程终结的是，内核必须释放它所占用的资源，并把这个不幸的消息告诉它的父进程。

在进程调用了do_exit（这个函数永不返回）之后，线程已经陷入僵死状态。
系统还是保留了它的pid。
这样做是为了让系统还有办法继续获取到它的信息。
对应的task_struct也还没有被释放。

# 4. 进程调度

调度程序没有太复杂的原理。
最大限度地利用处理器时间的原则是：
只要有可以执行的进程，那么就总会有进程正在执行。

但是只要系统中的进程数比核心数多，那么肯定某一时刻有进程不能被执行。
这些进程处于等待状态，从这些等待的进程中，选择一个来投入运行。
就是调度程序要做的事情。

## 4.1 多任务

多任务系统可以分为两种：
1、非抢占式。
2、抢占式。

抢占模式下，由调度程序决定什么时候把一个正在运行的程序强行停下来。
换另外一个进程去运行。
这个行为就叫抢占。
进程在被抢占之前可以运行的时间是预先设定好的。
而且有一个专门的名字，叫进程的时间片。

在非抢占模式下，除非进程主动停止运行，否则它会一直执行。
进程主动挂起自己的行为，叫让度yield。

## 4.2 Linux的进程调度
内核直到2.4版本，调度程序都是非常简陋的。
设计近乎原始。
当然它很容易理解。但是对于复杂场景难以胜任。
所以从2.5版本，开始对调度程序做大的修改。
使用一种叫O(1)的调度算法。
这种调度算法就是因为它的时间复杂度是常数而得名的。
它解决了之前Linux调度程序的很多的不足。
引入了许多强大的新特性。
O1调度器虽然在拥有数以十计的多处理的环境下表现近乎完美。但是也有缺点。
就是对于时间敏感的调度不及时。
对于桌面应用体验不佳。因为缺少交互进程。
所以在2.6版本，又继续对调度算法进行改进。
这种新的调度算法叫“完全公平调度算法”。简称CFS。我们接下来就是要看这种算法。

## 4.3 策略
调度器的策略往往就决定了系统的整体印象。
它是至关重要的。

进程优先级
调度算法里最基本的一种就是基于优先级的调度。
这是一种根据进程的价值和对cpu时间的需求来对进程分级的想法。
通常的做法是高优先级的先运行。低的后运行。
相同优先级的按照轮转方式进行调度。

Linux采用了两种不同的优先级范围。
1、nice值。
	nice值范围是-20到+19 。
	默认值为0 。
	nice值越大，优先级越低。
	（人越nice，越吃亏）

2、实时优先级。
	范围是0到99。这个是值越小优先级越低。
	任何实时进程的优先级都高于普通进程。
	

时间片
是一个数值，表示进程在被抢占之前可以持续运行的时间。
调度策略必须规定一个默认时间片。
但是这个不是一件容易的事情。
太长不行，太短也不行。
很多的os一般是10ms。
但是Linux没有直接这么做。
进程可以获得的处理器时间，跟系统负载关联起来。
还收到nice值的影响。

vi进程和视频处理进程，就是io进程和cpu进程的2个典型。

## 4.4 Linux调度算法

Linux的调度器是以模块的方式提供的。
不同的进程可以选择不同的调度算法。
每个调度器都有一个优先级。

CFS是一个针对普通进程的调度类。

现代的进程调度有2个基本概念：
1、进程优先级。
2、时间片。

CFS的出发点基于一个简单的理念：
进程调度的最终效果，应该如同系统具备一个完美的多任务处理器。

在理想的情况下，完美的多任务处理模型应该是这样的：
我们能在10ms内同时运行2个进程，他们各自使用处理器一半的能力。

当可运行的进程数量趋于无限的时候，它们各自获取的处理器时间都接近于0 。
这样就造成了不可接受的切换消耗。
CFS针对这一点引入了时间片底线。
这个底线被成为最小粒度。这个值默认是1ms。

## 4.5 Linux调度的实现
对应的代码在kernel/sched_fair.c里。
由4个部分组成：
1、时间记账。
2、进程选择。
3、调度器入口。
4、睡眠和唤醒。

CFS使用红黑树来组织进程队列。
这样就可以快速找到需要的进程。

当进程阻塞，或者结束的时候，就要从红黑树里移除。
所以树的操作是非常频繁的。

## 4.6 抢占和上下文切换

抢占分为：
1、用户抢占。
2、内核抢占。

## 4.7 实时调度策略
Linux提供了两种实时调度策略：
1、fifo。
	简单，先入先出。
	不基于时间片。
2、rr。
实时调度不受CFS管理。
实时调度代码在sched_rt.c里。

实时调度都是静态优先级。

## 4.8 与调度相关的系统调用

nice
	设置进程的nice值。
sched_setscheduler
sched_setparam
sched_get_priority_max
sched_rr_get_interval
sched_yield
sched_setaffinity

# 5. 系统调用
系统调用是用户进程和内核交互的接口。
这些接口让应用受限地访问硬件。
提供了创建新进程并与已有进程进行通信的机制。

提供这些接口是为了安全，避免应用乱来。

## 5.1 与内核通信

系统调用在app跟硬件之间添加了一个中间层。
这个层的作用主要有3个：
1、对app，它提供了硬件的抽象。
	例如写文件的时候，app不用关心硬件是什么介质。
2、保证系统的安全性。
3、为app提供虚拟空间。

## 5.2 API、POSIX和C库

在unix世界里，最流行的api是基于posix标准的。

## 5.3 系统调用

系统调用号
在Linux里，每一个系统调用都有一个编号。
系统调用号很重要，一旦分配就不能修改。

## 5.4 系统调用处理程序
app通过软中断通知内核。

## 5.5 系统调用的实现

## 5.6 系统调用上下文

# 6 内核数据结构

## 6.1 链表

## 6.2 队列
任何os内核都少不了一种编程模型：生产者和消费者。

kfifo

## 6.3 映射
idr。
内核里的map，不是通用的，但是简单实用。

## 6.4 二叉树

## 6.5 数据结构以及选择

## 6.6 算法复杂度

# 7 中断和中断处理

## 7.1 中断
## 7.2 中断处理程序
在Linux里，中断处理程序，就是简单的C函数。

## 7.3 上半部和下半部的对比
又要中断快，又要中断处理的事情多。
这个是矛盾的。
所以需要引入上半部和下半部。

以网卡为例。说明这种机制。
1、当网卡收到数据时，需要通知内核。
2、网卡需要立刻完成这件事情，这样网络吞吐量才能上去。
3、所以网卡马上发出中断，说，内核，我这有新数据。
4、内核里注册的中断响应函数开始被调用。
5、中断开始执行，把网卡的数据拷贝到内存。

## 7.4 注册中断处理程序

## 7.5 编写中断程序程序

## 7.6 中断上下文

中断上下文跟进程没有什么瓜葛。
跟current宏也没有什么关系。
因为没有后备进程，所以在中断上下文里是不能睡眠的。

中断处理的栈是一个可配置的项。
之前的中断处理程序没有自己单独的栈，使用了被中断的进程的栈。

在2.6版本的早期，增加了一个选项，可以把内核的栈由默认的2页（8KB）改为1页。
这样对内存的压力会更小。
因为每个进程的以前都需要两页连续且不可换出的内核内存。
为了应对这种内存的减小，中断程序需要有自己的栈了。
每个处理器一个栈，大小为一页。
这个栈就叫中断栈。

## 7.7 中断处理机制的实现

## 7.8 /proc/interrupts

## 7.9 中断控制

# 8 下半部和推后执行的工作

## 8.1 下半部
从硬件拷贝数据到内存，这个要在上半部里完成。
之后的要在内存里做。
这个下半部没有明确的规定。
完全取决于开发者的经验。

有些原则可供借鉴：
1、对时间很敏感的，在isr里处理。
2、和硬件相关，在isr里处理率。
3、要保证不被打断，在isr里处理。
4、其他的都可以放在下半部处理。

下半部有多种机制来实现。
下半部处理机制也经历了很多的变化。
有些名字起得不太好。

1、最早的Linux之提供bottom half这种机制来实现下半部。
	这个名字在那个时候是没有歧义的。
	因为是唯一的一种把工作推后的方法。机制机制简称BH。
	BH的接口也很简单：
		一个静态的、由32个bottom halves组成的链表。
	缺点是：
		1、即使属于不同的处理器，也不能同时2个BH同时执行。
	所以这种方法，简单，缺不够灵活，也有性能瓶颈。
2、于是，内核开发者就引入了任务队列。
	task queue。
	用来替代BH机制。
	这种机制还是不够灵活。
3、从2.3版本开始，内核开发者引入了软中断和tasklet。
	对于大部分下半部，用tasklet就可以了。
	tasklet的底层就是软中断。
4、从2.5版本开始，BH接口被废弃了。

## 8.2 软中断
## 8.3 tasklet
相比于软中断，tasklet的优点：
1、接口更简单。
2、锁保护要求较低。

每个处理器都有一组内核线程，用来帮助处理tasklet。
叫做ksoftirqd。

对于软中断，内核会选择在几个特殊的时机进行处理。
而最常见的是在isr返回的时候进行。
软中断被触发的频率有时可能很高。（例如大流量网络通信的时候）
更糟糕的是，处理函数有时候还会自行重复触发。
也就是说，软中断可以自己触发自己的再次执行（网络处理就是这么做的）。
这样就会导致app无法得到足够的cpu时间。
所以，这是个问题，需要解决。
我们先看看最容易想到的方案。有两种。
1、只要还有被触发且被等待处理的软中断，本地执行就要负责处理。
	这个不行，用户app难以得到运行时间。
2、不处理重新触发的软中断。
	这种方式也不太好，处理中断会不及时。
	
最后的解决方案是：
1、不立刻处理重新触发的软中断。
2、当大量软中断出现的时候，内核会唤醒一组内核线程来处理这些负载。
	这种线程的nice值是19 。
每个处理器都有一个这样的线程，ksoftirqd/n。

## 8.4 工作队列
work queue。
也是一种把工作推后执行的方式。
但是它还前面的几种都不同。
work queue可以把工作推后，交给一个内核线程去执行。

如何在work-queue和tasklet之间做出选择：
1、如果需要在处理中阻塞，那么就用work-queue。

work-queue的实现
work-queue子系统是一个用于创建kthread的接口。
它创建的内核线程叫worker-thread。
默认的work-thread叫events/n。

## 8.5 下半部机制的选择

## 8.6 在下半部之间加锁

使用tasklet的一个好处是：
它自己负责执行的序列化保障。
2个相同类型的tasklet不允许同时执行，即使在不同的处理器也不允许。

## 8.7 禁止下半部

# 9 内核同步介绍

## 9.1 临界区和竞争条件

## 9.2 加锁

## 9.3 死锁

## 9.4 争用和扩展性

# 10 内核同步方法

## 10.1 原子操作

## 10.2 自旋锁

## 10.3 读写自旋锁

## 10.4 信号量

## 10.5 读写信号量

## 10.6 mutex

## 10.7 completion

## 10.8 BLK：大内核锁

## 10.9 顺序锁

## 10.10 禁止抢占

## 10.11 顺序和屏障

# 11 定时器和时间管理

时间管理在内核中占有非常重要的地位。
内核中有大量的函数都是基于时间进行驱动的。

## 11.1 内核中的时间概念

## 11.2 节拍率：HZ

## 11.3 jiffies

全局变量jiffies用来记录自系统启动以来产生的节拍的总数。
启动时，这个变量的值为0 。
每次时钟中断加1 。

jiffies在32位机器上是32位，在64位机器上是64位。
32位的jiffies，时钟频率为100HZ时，497天后就会溢出。

jiffies溢出时的处理

## 11.4 硬时钟和定时器

## 11.5 时钟中断isr

## 11.6 实际时间

## 11.7 定时器

## 11.8 延迟执行

# 12 内存管理

在内核里分配内存不像在其他地方分配内存那么容易。
造成这种局面的因素有很多。
从根本上说，内核本身不能像用户空间那样奢侈地使用内存。

## 12.1 page

## 12.1 zone

## 12.3 获得page

## 12.4 kmalloc

## 12.5 vmalloc

## 12.6 slab

## 12.7 在栈上的静态分配

## 12.8 高端内存的映射

## 12.9 per-cpu的分配

## 12.10 新的per-cpu接口

## 12.11 使用per-cpu数据的原因

## 12.12 分配函数的选择

# 13 vfs

## 13.1 通用文件系统接口

## 13.2 文件系统抽象层

## 13.3 unix文件系统

## 13.4 vfs对象及其数据结构

## 13.5 超级块对象superblock

## 13.6 superblock操作

## 13.7 inode对象

## 13.8 inode操作

## 13.9 dentry对象

## 13.10 dentry操作

## 13.11 file对象

## 13.12 file操作

## 13.13 和文件系统相关的数据结构

## 13.14 和进程相关的数据结构

# 14 block io

## 14.1 剖析一个block设备

## 14.2 缓冲区和缓冲区头

## 14.3 bio结构体

## 14.4 请求队列

## 14.5 io调度程序

# 15 进程地址空间

内核除了管理自身的内存之外，还必须管理用户空间进程的内存。

我们称这个内存为进程地址空间。



## 15.1 地址空间

每个进程都有32位或者64位的平坦地址空间。



## 15.2 内存描述符mm_struct

内核使用内存描述符结构体来表示进程的地址空间。

这个结构体包含了和进程地址空间有关的全部信息。

通常，每个进程都有唯一的mm_struct结构体。也就是唯一的进程地址空间。

如果父进程希望跟子进程共享地址空间，那么在调用clone的时候，设置CLONE_VM标志。

这样的进程就是线程。

指定了CLONE_VM标志后，内核就不会调用allocate_mm函数。

只是在copy_mm的时候，把mm成员指向父进程的mm结构体就好。



进程退出的时候，内核调用exit_mm函数。



内核线程没有进程地址空间。mm这个成员就是NULL。

这也是内核线程的真实含义：它们没有用户上下文。



为了避免内核线程为mm_struct和page table浪费内存，也为了当新内核线程运行时，避免浪费CPU进行地址空间切换。

内核线程直接使用前一个进程的mm_struct。





## 15.3 vma

vma描述了一段连续的内存范围。

每个vma内部的内存，具有相同的属性，例如权限。



## 15.4 操作vma

## 15.5 mmap和do_mmap：创建vma

## 15.6 munmap和do_munmap：删除vma

## 15.7 页表


# 16 page cache和page writeback

page cache是对磁盘的内存进行缓存。
主要用来减少磁盘io操作。

具体来讲，就是把磁盘的数据缓存到内存里。

把对磁盘的访问转化为对内存的访问。

之所以需要page cache，是因为：

1、磁盘访问速度太慢。

2、临时局部原理。最近访问的数据，很有可能再次被访问。所以缓存就有意义。



## 16.1 缓存手段

page cache，从名字上看，就是跟内存的page有关系。

就是用内存的page，对应磁盘的块。

page cache可以根据内存的使用情况，动态调整，在内存不足的时候，减小page cache的使用。

当我们对文件进行read的时候，内核首先会检查page cache里是否有对应的数据，如果有，直接从page cache里取，这样就不去访问磁盘了。

这个情况，叫做缓存命中。

缓存不一定是整个文件，可以是文件的一部分。

上面说的是read的情况，那么write的情况是怎样呢？

write的使用，有3种缓存策略：

1、不缓存。直接写入磁盘。同时让page cache失效。很少用这种策略。

2、更新page cache，同时写入磁盘。这个叫做写穿（write through）。这种实现最简单。

3、write back策略。回写。Linux内核用的这种。write的时候，先只写入到page cache。不直接写磁盘。把对应的page cache标记为dirty。加入到dirty链表里。靠后台的一个回写进程来定时写入到磁盘。写入磁盘后，清除page cache的dirty标识。回写策略实现复杂一些，但是效率提高了很多。合并了写入操作，效率高。



缓存是如何进行调整大小的呢？

这个叫做缓存回收策略。

如果缓存中没有足够的干净page，内核强制触发回写操作。这样就可以腾出干净的page cache。

最关键是决定哪些page回收，对系统的影响最小？

1、LRU。最近最少使用的页。这个有缺陷。

2、双链策略。Linux内核使用的是这种策略。改进的LRU。维护2个LRU链表：active链表和inactive链表。



下面我们分析一下现实场景。

系统里有大量的文件被打开，

你编译代码的时候，缓存的文件使得编译过程访问磁盘次数更少，所以编译速度就更快了。

如果你的代码太多，无法一次性放入内存，那么就有一部分需要回收。





## 16.2 Linux page cache

page可以缓存普通文件、块设备文件、内存映射文件。

一个page可能包含多个不连续的磁盘块。

正是因为磁盘块的不连续，在page cache里检查某些数据是否已经被缓存，是已经困难的事情。

因为不能用设备名称和块号来做索引，否则这个就是最简单的方式。

Linux的page cache使用了address_space来管理缓存项和页IO操作。

address_space是vm_area_struct物理地址的对等体。



一个文件可以被10个（只是举例）vm_area_struct结构体标识。（例如有5个进程，每个进程了调用了2次mmap来映射这个文件）

但是这个文件只对应一个address_space结构体。

也就是说，文件可以有多个虚拟地址，但是物理地址只有一个。

address_space这个名字取得并不好，更符合本质的名字应该是physical_pages_of_a_file。



因为任何IO操作之前，内核都要检查一下page是否已经在page cache里了。

所以这种检查是非常频繁的。所以它的效率就是很关键的。

至少可以保证在缓存命中率很低的时候，仍然比不缓存要快。

内核在address_space里有一个radix tree字段，就是用来实现这种高效检索的。

在2.6版本以前，内核是使用一个全局散列表来做这个事情的。这个效率是很低的。





## 16.3 缓冲区高速缓存

## 16.4 flusher线程

回写的时机：

1、空闲内存低于某个阈值时。

2、dirty页在内存里驻留的时间超过某个阈值时。

3、用户进程调用sync和fsync的时候。

在笔记本电脑上运行的内核，会执行一种特殊的回写策略。

因为笔记本电脑对功耗比较敏感。所以希望硬盘尽量不要工作。

/proc/sys/vm/laptop_mode 这个就是控制的。默认是0 。

回写的时间间隔更大一些，例如10分钟。

坏处是意外崩溃时，可能丢数据。

一般会检测电源插入状态，动态调整策略。





# 17 设备与模块

## 17.1 设备类型

Linux的设备分为3种：
1、字符设备。
2、块设备。
3、网络设备。

块设备一般称为blkdev。寻址以块为单位，块的大小根据不同设备类型不同而不同。

块设备一般支持seeking操作。

就是可以对数据进行随机访问。

字符设备，是不可以寻址的，只能进行流式访问。

例如键盘，鼠标。





## 17.2 模块

## 17.3 设备模型

## 17.4 sysfs

# 18 调试

# 19 可移植性


