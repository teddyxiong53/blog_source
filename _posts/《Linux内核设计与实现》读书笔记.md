---
title: 《Linux内核设计与实现》读书笔记
date: 2019-11-30 16:48:31
tags:
	- Linux

---

接口定义合理，代码风格一致。
一次做一件事情，做到完美。
这是Linus倡导的内核的开发原则。

# 1 Linux内核简介

## 1.1 Unix的历史
Unix取得成功的原因：
1、unix很简洁。
	不想其他的系统，有上千个syscall。
	unix只有200多个。而且用途非常明确。
2、unix里，一切都是文件。
	这样处理接口就可以统一。
3、使用C语言编写。
	这样可移植性就很高。
4、unix创建进程很快。
	fork的优点。
5、简单高效的ipc方式。
	
## 1.2 追寻Linus足迹：Linux简介

## 1.3 操作系统和内核简介

## 1.4 Linux内核与传统Unix内核比较
Linux是宏内核，也就是说，Linux内核运行在单独的内核地址空间上。
不过，Linux吸收了微内核的优点：
1、模块化设计。
2、抢占式内核。
3、支持内核线程。
4、动态加载内核模块的能力。
Linux避免了微内核设计上性能的缺陷。
让所有的事情都运行在内核态，直接调用函数，无须消息传递。

Linux是实用主义的胜利。

## 1.5 Linux内核版本

## 1.6 Linux内核开发社区

# 2. 从内核出发

## 2.1 获取内核源代码

## 2.2 内核源码树

## 2.3 编译内核

## 2.4 内核开发的特点

1、内核编程不能使用C库函数和C库头文件。
	很多函数在内核里有一个简单的实现。
2、内核编程必须使用GNU C。
	内联函数
	内联汇编
	分支声明unlikely这种。
	
3、内核编程缺乏内存保护机制。
	内存错误会导致oops。
	
4、内核编码不要使用浮点计算。
	内核不能完美地支持浮点操作。
	
5、内核里每个进程只有一个很小的定长的栈。
	32位的是8K。64位的是16K。
	
6、因为内核支持异步中断、强制和smp，所以需要随时注意同步和并发。
7、要考虑可移植性。

# 3. 进程管理

## 3.1 进程

每个线程有一个独立的程序计数器、进程栈、一组进程寄存器。

内核调度的对象是线程，而不是进程。

Linux实现的线程其实是一种特殊的进程。

在现代os里，进程提供两种虚拟机制：
1、虚拟处理器。
2、虚拟内存。

## 3.2 进程描述符和任务结构

内核把进程的列表存放在叫task list的双向循环链表里。
链表里的每一项都是一个task_struct。
里面存放了一个进程的所有信息。
task_struct结构体比较大。
在32位机器上，大概是1.7KB。

Linux通过slab分配器来分配task_struct。
这样可以达到2个目的：
1、对象复用。
2、缓存着色。

在进程的内核栈的底部，有一个struct thread_info，里面有一个task_struct指针。

pid的存放
pid默认是32768，这个对于桌面系统够用了。
之所以是这个值，是为了跟老的系统兼容，之前是short类型的。
对于服务器是不够的。
可以通过修改/proc/sys/kernel/pid_max来修改。

内核中大部分处理进程的代码，都是通过task_struct来做的。

因此，通过current宏查找到当前正在运行的进程的pid就很主要。
根据硬件的不同，current宏的实现也不同。

进程的状态，有5种：
1、running。
2、interruptible。
	睡眠中，可以被提前唤醒。
3、uninterruptible。
	睡眠中，不能被提前唤醒。
4、traced。
	正在被追踪，例如被ptrace追踪。
5、stopped。
	收到STOP信号时。
	调试期间也是这种状态。
	
进程家族树
所有的用户进程都是pid为1的init进程的后代。

## 3.3 进程的创建

Linux通过clone这个syscall来实现fork函数。
clone有一些标志。
fork、vfork、__clone这些函数都是调用了clone，只是设置的标志不一样。

vfork
除了不拷贝父进程的页表项之外，vfork和fork功能相同。

## 3.4 线程在Linux中的实现
线程机制是现代编程技术中常用的一种抽象机制。
这种机制提供了在同一进程地址空间进行并发的能力。

从Linux内核的角度说，并不存在线程的概念。
所有的线程都是进程。


内核线程
内核经常需要在后台执行一些操作。
这种任务通过内核线程来完成。

内核线程跟普通进程的区别：
内核线程没有独立的地址空间，task_struct的mm指针为NULL。
内核进程和普通进程一样，可以被调度和抢占。

内核确实是把一些任务交给内核线程去做。
向flush和ksofirqd。
你用top可以看到不少的内核线程。

## 3.5 进程终结

当一个进程终结的是，内核必须释放它所占用的资源，并把这个不幸的消息告诉它的父进程。

在进程调用了do_exit（这个函数永不返回）之后，线程已经陷入僵死状态。
系统还是保留了它的pid。
这样做是为了让系统还有办法继续获取到它的信息。
对应的task_struct也还没有被释放。

# 4. 进程调度

调度程序没有太复杂的原理。
最大限度地利用处理器时间的原则是：
只要有可以执行的进程，那么就总会有进程正在执行。

但是只要系统中的进程数比核心数多，那么肯定某一时刻有进程不能被执行。
这些进程处于等待状态，从这些等待的进程中，选择一个来投入运行。
就是调度程序要做的事情。

## 4.1 多任务

多任务系统可以分为两种：
1、非抢占式。
2、抢占式。

抢占模式下，由调度程序决定什么时候把一个正在运行的程序强行停下来。
换另外一个进程去运行。
这个行为就叫抢占。
进程在被抢占之前可以运行的时间是预先设定好的。
而且有一个专门的名字，叫进程的时间片。

在非抢占模式下，除非进程主动停止运行，否则它会一直执行。
进程主动挂起自己的行为，叫让度yield。

## 4.2 Linux的进程调度
内核直到2.4版本，调度程序都是非常简陋的。
设计近乎原始。
当然它很容易理解。但是对于复杂场景难以胜任。
所以从2.5版本，开始对调度程序做大的修改。
使用一种叫O(1)的调度算法。
这种调度算法就是因为它的时间复杂度是常数而得名的。
它解决了之前Linux调度程序的很多的不足。
引入了许多强大的新特性。
O1调度器虽然在拥有数以十计的多处理的环境下表现近乎完美。但是也有缺点。
就是对于时间敏感的调度不及时。
对于桌面应用体验不佳。因为缺少交互进程。
所以在2.6版本，又继续对调度算法进行改进。
这种新的调度算法叫“完全公平调度算法”。简称CFS。我们接下来就是要看这种算法。

## 4.3 策略
调度器的策略往往就决定了系统的整体印象。
它是至关重要的。

进程优先级
调度算法里最基本的一种就是基于优先级的调度。
这是一种根据进程的价值和对cpu时间的需求来对进程分级的想法。
通常的做法是高优先级的先运行。低的后运行。
相同优先级的按照轮转方式进行调度。

Linux采用了两种不同的优先级范围。
1、nice值。
	nice值范围是-20到+19 。
	默认值为0 。
	nice值越大，优先级越低。
	（人越nice，越吃亏）

2、实时优先级。
	范围是0到99。这个是值越小优先级越低。
	任何实时进程的优先级都高于普通进程。
	

时间片
是一个数值，表示进程在被抢占之前可以持续运行的时间。
调度策略必须规定一个默认时间片。
但是这个不是一件容易的事情。
太长不行，太短也不行。
很多的os一般是10ms。
但是Linux没有直接这么做。
进程可以获得的处理器时间，跟系统负载关联起来。
还收到nice值的影响。

vi进程和视频处理进程，就是io进程和cpu进程的2个典型。

## 4.4 Linux调度算法

Linux的调度器是以模块的方式提供的。
不同的进程可以选择不同的调度算法。
每个调度器都有一个优先级。

CFS是一个针对普通进程的调度类。

现代的进程调度有2个基本概念：
1、进程优先级。
2、时间片。

CFS的出发点基于一个简单的理念：
进程调度的最终效果，应该如同系统具备一个完美的多任务处理器。

在理想的情况下，完美的多任务处理模型应该是这样的：
我们能在10ms内同时运行2个进程，他们各自使用处理器一半的能力。

当可运行的进程数量趋于无限的时候，它们各自获取的处理器时间都接近于0 。
这样就造成了不可接受的切换消耗。
CFS针对这一点引入了时间片底线。
这个底线被成为最小粒度。这个值默认是1ms。

## 4.5 Linux调度的实现
对应的代码在kernel/sched_fair.c里。
由4个部分组成：
1、时间记账。
2、进程选择。
3、调度器入口。
4、睡眠和唤醒。

CFS使用红黑树来组织进程队列。
这样就可以快速找到需要的进程。

当进程阻塞，或者结束的时候，就要从红黑树里移除。
所以树的操作是非常频繁的。

## 4.6 抢占和上下文切换

抢占分为：
1、用户抢占。
2、内核抢占。

## 4.7 实时调度策略
Linux提供了两种实时调度策略：
1、fifo。
	简单，先入先出。
	不基于时间片。
2、rr。
实时调度不受CFS管理。
实时调度代码在sched_rt.c里。

实时调度都是静态优先级。

## 4.8 与调度相关的系统调用

nice
	设置进程的nice值。
sched_setscheduler
sched_setparam
sched_get_priority_max
sched_rr_get_interval
sched_yield
sched_setaffinity

# 5. 系统调用
系统调用是用户进程和内核交互的接口。
这些接口让应用受限地访问硬件。
提供了创建新进程并与已有进程进行通信的机制。

提供这些接口是为了安全，避免应用乱来。

## 5.1 与内核通信

系统调用在app跟硬件之间添加了一个中间层。
这个层的作用主要有3个：
1、对app，它提供了硬件的抽象。
	例如写文件的时候，app不用关心硬件是什么介质。
2、保证系统的安全性。
3、为app提供虚拟空间。

## 5.2 API、POSIX和C库

在unix世界里，最流行的api是基于posix标准的。

## 5.3 系统调用

系统调用号
在Linux里，每一个系统调用都有一个编号。
系统调用号很重要，一旦分配就不能修改。

## 5.4 系统调用处理程序
app通过软中断通知内核。

## 5.5 系统调用的实现

## 5.6 系统调用上下文

# 6 内核数据结构

## 6.1 链表

## 6.2 队列
任何os内核都少不了一种编程模型：生产者和消费者。

kfifo

## 6.3 映射
idr。
内核里的map，不是通用的，但是简单实用。

## 6.4 二叉树

## 6.5 数据结构以及选择

## 6.6 算法复杂度

# 7 中断和中断处理

## 7.1 中断
## 7.2 中断处理程序
在Linux里，中断处理程序，就是简单的C函数。

## 7.3 上半部和下半部的对比
又要中断快，又要中断处理的事情多。
这个是矛盾的。
所以需要引入上半部和下半部。

以网卡为例。说明这种机制。
1、当网卡收到数据时，需要通知内核。
2、网卡需要立刻完成这件事情，这样网络吞吐量才能上去。
3、所以网卡马上发出中断，说，内核，我这有新数据。
4、内核里注册的中断响应函数开始被调用。
5、中断开始执行，把网卡的数据拷贝到内存。

## 7.4 注册中断处理程序

## 7.5 编写中断程序程序

## 7.6 中断上下文

中断上下文跟进程没有什么瓜葛。
跟current宏也没有什么关系。
因为没有后备进程，所以在中断上下文里是不能睡眠的。

中断处理的栈是一个可配置的项。
之前的中断处理程序没有自己单独的栈，使用了被中断的进程的栈。

在2.6版本的早期，增加了一个选项，可以把内核的栈由默认的2页（8KB）改为1页。
这样对内存的压力会更小。
因为每个进程的以前都需要两页连续且不可换出的内核内存。
为了应对这种内存的减小，中断程序需要有自己的栈了。
每个处理器一个栈，大小为一页。
这个栈就叫中断栈。

## 7.7 中断处理机制的实现

## 7.8 /proc/interrupts

## 7.9 中断控制

# 8 下半部和推后执行的工作

## 8.1 下半部
从硬件拷贝数据到内存，这个要在上半部里完成。
之后的要在内存里做。
这个下半部没有明确的规定。
完全取决于开发者的经验。

有些原则可供借鉴：
1、对时间很敏感的，在isr里处理。
2、和硬件相关，在isr里处理率。
3、要保证不被打断，在isr里处理。
4、其他的都可以放在下半部处理。

下半部有多种机制来实现。
下半部处理机制也经历了很多的变化。
有些名字起得不太好。

1、最早的Linux之提供bottom half这种机制来实现下半部。
	这个名字在那个时候是没有歧义的。
	因为是唯一的一种把工作推后的方法。机制机制简称BH。
	BH的接口也很简单：
		一个静态的、由32个bottom halves组成的链表。
	缺点是：
		1、即使属于不同的处理器，也不能同时2个BH同时执行。
	所以这种方法，简单，缺不够灵活，也有性能瓶颈。
2、于是，内核开发者就引入了任务队列。
	task queue。
	用来替代BH机制。
	这种机制还是不够灵活。
3、从2.3版本开始，内核开发者引入了软中断和tasklet。
	对于大部分下半部，用tasklet就可以了。
	tasklet的底层就是软中断。
4、从2.5版本开始，BH接口被废弃了。

## 8.2 软中断
## 8.3 tasklet
相比于软中断，tasklet的优点：
1、接口更简单。
2、锁保护要求较低。

每个处理器都有一组内核线程，用来帮助处理tasklet。
叫做ksoftirqd。

对于软中断，内核会选择在几个特殊的时机进行处理。
而最常见的是在isr返回的时候进行。
软中断被触发的频率有时可能很高。（例如大流量网络通信的时候）
更糟糕的是，处理函数有时候还会自行重复触发。
也就是说，软中断可以自己触发自己的再次执行（网络处理就是这么做的）。
这样就会导致app无法得到足够的cpu时间。
所以，这是个问题，需要解决。
我们先看看最容易想到的方案。有两种。
1、只要还有被触发且被等待处理的软中断，本地执行就要负责处理。
	这个不行，用户app难以得到运行时间。
2、不处理重新触发的软中断。
	这种方式也不太好，处理中断会不及时。
	
最后的解决方案是：
1、不立刻处理重新触发的软中断。
2、当大量软中断出现的时候，内核会唤醒一组内核线程来处理这些负载。
	这种线程的nice值是19 。
每个处理器都有一个这样的线程，ksoftirqd/n。

## 8.4 工作队列
work queue。
也是一种把工作推后执行的方式。
但是它还前面的几种都不同。
work queue可以把工作推后，交给一个内核线程去执行。

如何在work-queue和tasklet之间做出选择：
1、如果需要在处理中阻塞，那么就用work-queue。

work-queue的实现
work-queue子系统是一个用于创建kthread的接口。
它创建的内核线程叫worker-thread。
默认的work-thread叫events/n。

## 8.5 下半部机制的选择

## 8.6 在下半部之间加锁

使用tasklet的一个好处是：
它自己负责执行的序列化保障。
2个相同类型的tasklet不允许同时执行，即使在不同的处理器也不允许。

## 8.7 禁止下半部

# 9 内核同步介绍

## 9.1 临界区和竞争条件

## 9.2 加锁

## 9.3 死锁

## 9.4 争用和扩展性

# 10 内核同步方法

## 10.1 原子操作

## 10.2 自旋锁

## 10.3 读写自旋锁

## 10.4 信号量

## 10.5 读写信号量

## 10.6 mutex

## 10.7 completion

## 10.8 BLK：大内核锁

## 10.9 顺序锁

## 10.10 禁止抢占

## 10.11 顺序和屏障

# 11 定时器和时间管理

时间管理在内核中占有非常重要的地位。
内核中有大量的函数都是基于时间进行驱动的。

## 11.1 内核中的时间概念

## 11.2 节拍率：HZ

## 11.3 jiffies

全局变量jiffies用来记录自系统启动以来产生的节拍的总数。
启动时，这个变量的值为0 。
每次时钟中断加1 。

jiffies在32位机器上是32位，在64位机器上是64位。
32位的jiffies，时钟频率为100HZ时，497天后就会溢出。

jiffies溢出时的处理

## 11.4 硬时钟和定时器

## 11.5 时钟中断isr

## 11.6 实际时间

## 11.7 定时器

## 11.8 延迟执行

# 12 内存管理

在内核里分配内存不像在其他地方分配内存那么容易。
造成这种局面的因素有很多。
从根本上说，内核本身不能像用户空间那样奢侈地使用内存。

## 12.1 page

## 12.1 zone

## 12.3 获得page

## 12.4 kmalloc

## 12.5 vmalloc

## 12.6 slab

## 12.7 在栈上的静态分配

## 12.8 高端内存的映射

## 12.9 per-cpu的分配

## 12.10 新的per-cpu接口

## 12.11 使用per-cpu数据的原因

## 12.12 分配函数的选择

# 13 vfs

## 13.1 通用文件系统接口

## 13.2 文件系统抽象层

## 13.3 unix文件系统

## 13.4 vfs对象及其数据结构

## 13.5 超级块对象superblock

## 13.6 superblock操作

## 13.7 inode对象

## 13.8 inode操作

## 13.9 dentry对象

## 13.10 dentry操作

## 13.11 file对象

## 13.12 file操作

## 13.13 和文件系统相关的数据结构

## 13.14 和进程相关的数据结构

# 14 block io

## 14.1 剖析一个block设备

## 14.2 缓冲区和缓冲区头

## 14.3 bio结构体

## 14.4 请求队列

## 14.5 io调度程序

# 15 进程地址空间

## 15.1 地址空间

## 15.2 内存描述符mm_struct

## 15.3 vma

## 15.4 操作vma

## 15.5 mmap和do_mmap：创建vma

## 15.6 munmap和do_munmap：删除vma

## 15.7 页表


# 16 page cache和page writeback

page cache是对磁盘的内存进行缓存。
主要用来减少磁盘io操作。
## 16.1 缓存手段

## 16.2 Linux page cache

## 16.3 缓冲区高速缓存

## 16.4 flusher线程

# 17 设备与模块

## 17.1 设备类型

Linux的设备分为3种：
1、字符设备。
2、块设备。
3、网络设备。

## 17.2 模块

## 17.3 设备模型

## 17.4 sysfs

# 18 调试

# 19 可移植性


