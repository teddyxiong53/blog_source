---
title: npu之softmax
date: 2021-08-27 14:13:33
tags:
	- npu

---

--

在机器学习尤其是深度学习中，

softmax是个非常常用而且比较重要的函数，

尤其在多分类的场景中使用广泛。

他把一些输入映射为0-1之间的实数，并且归一化保证和为1，

因此多分类的概率之和也刚好为1。



首先我们简单来看看softmax是什么意思。

顾名思义，softmax由两个单词组成，

其中一个是max。

对于max我们都很熟悉，

比如有两个变量a,b。如果a>b，则max为a，反之为b。

用伪码简单描述一下就是 if a > b return a; else b。

另外一个单词为soft。

max存在的一个问题是什么呢？

如果将max看成一个分类问题，就是非黑即白，

最后的输出是一个确定的变量。

更多的时候，我们希望输出的是取到某个分类的概率，

或者说，我们希望分值大的那一项被经常取到，

而分值较小的那一项也有一定的概率偶尔被取到，

所以我们就应用到了soft的概念，

即最后的输出是每个分类被取到的概率。



参考资料

1、小白都能看懂的softmax详解

https://blog.csdn.net/bitcarmanlee/article/details/82320853