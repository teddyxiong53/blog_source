---
title: npu之cnn
date: 2021-08-23 13:24:33
tags:
	- npu

---

--

卷积是大自然中最常见的运算，

一切信号观测、采集、传输、处理都可以用卷积过程实现。

例如，你拍照时手抖了一下，导致照片模糊，

实际上等价于手没抖拍摄的正常照片与一个表示手抖的卷积核进行卷积运算得到的结果。

用公式表达如下：



# 卷积的理解

![img](../images/random_name/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy92STluWWU5NGZzSExUdWZPcTl5WHZic0xwaWF5TWpyYTVkeEVqclR0YWlhY3RnbUxrM29iSWIxVDlDTmI0WXRPZ2JZNDZkMDgzcHdYMWgzbjBwSzFpY1RTdy82NDA_d3hfZm10PXBuZw)

先对g函数进行翻转，相当于在数轴上把g函数从右边褶到左边去，也就是卷积的“卷”的由来。

然后再把g函数平移到n，在这个位置对两个函数的对应点**相乘，然后相加**，这个过程是卷积的“积”的过程。

这个只是从计算的方式上对公式进行了解释，

从数学上讲无可挑剔，

但进一步追问，**为什么要先翻转再平移，**

这么设计有何用意？还是有点费解。

为了更好地理解这些问题，我们先给出两个典型的应用场景：

**1. 信号分析**

一个输入信号*f(t)*，经过一个线性系统（其特征可以用单位冲击响应函数*g(t)*描述）以后，输出信号应该是什么？实际上通过卷积运算就可以得到输出信号。

**2. 图像处理**

输入一幅图像*f(x,y)*，经过特定设计的卷积核*g(x,y)*进行卷积处理以后，输出图像将会得到模糊，边缘强化等各种效果。

# 概述

因为图像的维度普遍比较高，例如 MNIST 数据集，每一个图片是 28 * 28 的图片。

如果直接用神经网络，假设采用2个 1000个神经元的隐藏层加 1 个10个神经元的隐藏层，最后使用 softmax 分类层，输出 10 个数字对应的概率。

参数的数量有：

786 * 1000 * 1000 * 10

如果是更大一点的图片，网络的规模还会进一步快速的增长。

为了应对这种问题， Yann LeCun 在贝尔实验室做研究员的时候提出了卷积网络技术，

并展示如何使用它来大幅度提高手写识别能力。

接下来将介绍卷积和池化以及卷积神经网络。

使用CNN的整体优势在于，

它可以使用其内核从数据中提取空间特征，

而其他网络则无法做到。

例如，CNN可以检测图像中的边缘，颜色分布等，

这使得这些网络在图像分类和包含空间属性的其他类似数据中非常强大。



## 参考资料

https://tensornews.cn/intro_cnn/

https://blog.csdn.net/orDream/article/details/106342711



# 卷积神经网络的应用

## 影像辨识

卷积神经网络通常在图像分析（image analysis）和图像处理（image processing）领域中使用。

关系密切，两者有一定程度的交叉，但是又有所不同。

图像处理侧重于信号处理方面的研究，

比如图像对比度的调节、图像编码、去噪以及各种滤波的研究。

但是图像分析更侧重点在于研究图像的内容，

包括但不局限于使用图像处理的各种技术，

它更倾向于对图像内容的分析、解释、和识别。

因而，图像分析和计算机科学领域中的模式识别、计算机视觉关系更密切一些。



图像分析研究的领域一般包括：

基于内容的图像检索（CBIR-Content Based Image Retrieval）
人脸识别（face recognition）
表情识别（emotion recognition）
光学字符识别（OCR-Optical Character Recognition）
手写体识别（handwriting recognition）
医学图像分析（biomedical image analysis）



## 自然语言处理

卷积神经网络也常被用于自然语言处理。 

CNN的模型被证明可以有效的处理各种自然语言处理的问题，

如语义分析、搜索结果提取、句子建模、分类、预测、和其他传统的NLP任务等。

自然语言处理（英语：natural language processing，缩写作NLP）

是人工智能和语言学领域的分支学科。

此领域探讨如何处理及运用自然语言；

自然语言认知则是指让电脑“懂”人类的语言。

自然语言生成系统把计算机数据转化为自然语言。

自然语言理解系统把自然语言转化为计算机程序更易于处理的形式。

自然语言处理研究的领域一般包括：

文本朗读（Text to speech）/语音合成（Speech synthesis）
语音识别（Speech recognition）
中文自动分词（Chinese word segmentation）
词性标注（Part-of-speech tagging）
句法分析（Parsing）
自然语言生成（Natural language generation）
文本分类（Text categorization）
信息检索（Information retrieval）
信息抽取（Information extraction）
文字校对（Text-proofing）
问答系统（Question answering）

给一句人类语言的问句，决定其答案。 典型问题有特定答案 (像是加拿大的首都叫什么?)，但也考虑些开放式问句(像是人生的意义是是什么?)

机器翻译（Machine translation）
将某种人类语言自动翻译至另一种语言

自动摘要（Automatic summarization）
产生一段文字的大意，通常用于提供已知领域的文章摘要，例如产生报纸上某篇文章之摘要

文字蕴涵（Textual entailment）



首先，介绍一下什么是卷积神经网络。

它是使用卷积层（Convolutional layers）的神经网络，基于卷积的数学运算。

卷积层由一组滤波器组成，滤波器可以视为二维数字矩阵。这是一个示例3x3滤波器：

我们可以将滤波器与输入图像进行卷积来产生输出图像，那么什么是卷积操作呢？具体的步骤如下：

1. 在图像的某个位置上覆盖滤波器；
2. 将滤波器中的值与图像中的对应像素的值相乘；
3. 把上面的乘积加起来，得到的和是输出图像中目标像素的值；
4. 对图像的所有位置重复此操作。



![img](../images/random_name/3232548-ad8c1ead78877d28.gif)

下面动图就是一个卷积的过程。移动的那个小方块就是过滤器。

![img](../images/random_name/3232548-3bf21864a5897262.gif)



卷积的作用主要是：

通过卷积运算，可以使原**信号特征增强，并且降低噪音**。

在图像上卷积之后主要是减少图像噪声，提取图像的特征。

例如sobel算子就是一种卷积运算，主要是提取图像的边缘特征。

卷积网络能很好地**适应图像的平移不变性**：

例如稍稍移动一幅猫的图像，它仍然是一幅猫的图像。

卷积操作保留了图像块之间的空间信息，进行卷积操作的图像块之间的相对位置关系没有改变。

下面图片演示了不同的卷积核产生的效果。

![img](../images/random_name/3232548-f59ee728ee6aac44.png)



看出来了吗？其实，**索伯滤波器是是边缘检测器**。

现在可以解释卷积操作的用处了：

用输出图像中更亮的像素表示原始图像中存在的边缘。

你能看出为什么边缘检测图像可能比原始图像更有用吗？

回想一下MNIST手写数字分类问题。

在MNIST上训练的CNN可以找到某个特定的数字。

**比如发现数字1，可以通过使用边缘检测发现图像上两个突出的垂直边缘。**

通常，卷积有助于我们找到特定的局部图像特征（如边缘），用在后面的网络中。



在上面的处理过程中，我们用3x3滤波器对4x4输入图像执行卷积，输出了一个2x2图像。



# 池化

Pooling有多种，这里主要介绍两种，

max-pooling和average-pooling。

max-pooling即为从四个元素中选取一个最大的来表示这四个元素，

average-pooling则用四个元素的平均值来表示这四个元素。



图像中的相邻像素倾向于具有相似的值，

因此通常卷积层相邻的输出像素也具有相似的值。

这意味着，卷积层输出中包含的大部分信息都是冗余的。

如果我们使用边缘检测滤波器并在某个位置找到强边缘，那么我们也可能会在距离这个像素1个偏移的位置找到相对较强的边缘。

但是它们都一样是边缘，我们并没有找到任何新东西。

池化层解决了这个问题。

这个网络层所做的就是通过减小输入的大小**降低输出值的数量**。

**池化一般通过简单的最大值、最小值或平均值操作完成。**

以下是池大小为2的最大池层的示例:

![img](../images/random_name/v2-ac441205fd06dc037b3db2dbf05660f7_720w.webp)



卷积本身就是黑箱，试图解释本来就是拟合这个过程，并不能解释完美。

**卷积如同管中窥豹，缩小了范围，不同的镜片和镜头大小如同不同的卷积核。**

一维卷积如同管中听声音，

二维卷积如同管中看平面世界，

三维卷积如同管中看立体世界，

四维卷积如同管中看有颜色的立体世界，

五维卷积如同管中看有颜色的立体世界+声音。。。

**卷积只见到事物的一小部分，看不到全部，**

**这时需要池化来加强统计，**

**神经网络来建立联系，**

主要目的是减少计算量，

计算量可以并行化处理，

计算机的并行运算单元可以拿好多个管同时来看豹，

这样并行化程序就高了，

再用神经网络训练把这些小范围的数据联系起来，

从更高层面上运算。

像小波变换中的小波基函数也是类似这种，SVM支持向量机中的核函数也是类似这种，最早来原于滤波器函数、傅立叶变换函数。

有类似液晶显示器的栅格化算法，

化复杂为简单，

无论多复杂的东西都可以实现在一个有限的张量表示上，

只要密度上去了可以无限逼近原始解，

有限元分析也类似，

用有限数量的已知量去逼近无限未知量，

数学中有类似的勒贝格积分，像信号处理中的希尔伯特黄变换也有类似思想。





## 参考资料

卷积层

https://baike.baidu.com/item/%E5%8D%B7%E7%A7%AF%E5%B1%82/22701737?fr=aladdin

如何理解卷积神经网络（CNN）中的卷积和池化？

https://www.zhihu.com/question/49376084

# CNN架构演进

CNN从90年代的LeNet开始，

21世纪初沉寂了10年，

直到12年AlexNet开始又再焕发第二春，

从ZF Net到VGG，GoogLeNet再到ResNet和最近的DenseNet，

网络越来越深，架构越来越复杂，

解决反向传播时梯度消失的方法也越来越巧妙。

新年有假期，就好好总结一波CNN的各种经典架构吧，

领略一下CNN的发展历程中各路大神之间的智慧碰撞之美。

本文将会谈到以下经典的卷积神经网络：

1. LeNet
2. AlexNet
3. ZF
4. VGG
5. GoogLeNet
6. ResNet
7. DenseNet

## 开山之作：LeNet

闪光点：定义了CNN的基本组件，是CNN的鼻祖。

LeNet是卷积神经网络的祖师爷LeCun在1998年提出，

用于解决手写数字识别的视觉任务。

自那时起，CNN的最基本的架构就定下来了：

卷积层、池化层、全连接层。

如今各大深度学习框架中所使用的LeNet都是**简化改进过的LeNet-5**（-5表示具有5个层），

和原始的LeNet有些许不同，

比如把激活函数改为了现在很常用的ReLu。

LeNet-5跟现有的conv->pool->ReLU的套路不同，

它使用的方式是conv1->pool->conv2->pool2再接全连接层，

但是不变的是，卷积层后紧接池化层的模式依旧不变。



## 参考资料

CNN网络架构演进：从LeNet到DenseNet

https://www.cnblogs.com/skyfsm/p/8451834.html

# ARM-NN

物联网已经深入到我们生活的方方面面，

例如穿戴式医疗设备、智能家居。

大部分的物联网设备会将采集到的数据上传至云端，由后台进行数据处理和分析，再将结果返回给微处理器。

然而这种云端处理数据的方式不适用于一些对实时性要求高的物联网边缘设备，

通过ARM Cortex-M系列处理器内核进行Machine Learning成为了技术发展的需要，

CMSIS-NN就是解决方法之一。

机器学习运用到嵌入式系统中有以下的优点：

实时性决策
增加数据的安全性和可靠性
无需依赖互联网
减少带宽

CMSIS-NN库分为了两个部分，NNFuctions和NNSupportFunctions，图片摘自CMSIS-NN Block Diagram。

![在这里插入图片描述](../images/random_name/20210207145630565.png)

NNFuctions运用了卷积神经网络Convolutional Neural Network(CNN)，包括了：

Convolution Function 卷积函数
Activations Function 激活函数
Pooling Function 池化函数
Fully-connected Function 全连接函数

要掌握CMSIS-NN，首先得理解CNN，所以先说一说卷积神经网络。

Convolutional Neural Network



卷积是怎么卷的？

**卷积的核心在于降维。**



当我们遍历这条直线，也就是做积分或者是求和，像是把二维平面从45°斜线卷起来，就形成了一个一维的直线函数，每个点的函数值等于卷起来后重合的点的函数值之和。这样就从一个二维函数降到了一维函数。


常用的激活函数有Sigmoid、tanh以及ReLu。

Sigmoid和tanh函数常用于全连接层，

ReLu函数常用于卷积层。

池化函数的主要作用是通过特征降维对图像进行压缩，降低feature maps的分辨率，以此减少计算量，同时提高容错率。

池化一般选取2x2的滑动窗口作为池化区域，通过相应的池化函数将4个像素转换为1个像素。

常用的池化函数有MaxPooling和AveragePooling。



CNN算法可以分为两个部分：

- 特征提取：Convolution、Activation、Pooling
- 分类识别：Fully-connected



CMSIS-NN采用的是框架如下图。

![在这里插入图片描述](../images/random_name/20210224173051356.png)



使用了以下函数：

arm_convolve_HWC_q7_RGB()
arm_convolve_HWC_q7_fast()
arm_relu_q7()
arm_maxpool_q7_HWC()
arm_avepool_q7_HWC()
arm_fully_connected_q7_opt()
arm_fully_connected_q7()



# 参考资料

1、CMSIS_NN：卷积神经网络

https://blog.csdn.net/weixin_42150654/article/details/113741171

2、【CNN】很详细的讲解什么以及为什么是卷积（Convolution）！

https://blog.csdn.net/zandaoguang/article/details/103640599