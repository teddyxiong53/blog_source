---
title: npu之深度学习的batch理解
date: 2021-09-29 11:12:33
tags:
	- npu

---

--

Batch_Size（批尺寸）是机器学习中一个重要参数，涉及诸多矛盾，下面逐一展开。

首先，为什么需要有 Batch_Size 这个参数？

Batch 的选择，首先决定的是下降的方向。

如果数据集比较小，完全可以采用**全数据集 （ Full Batch Learning ）**的形式，

这样做**至少**有 2 个好处：

其一，由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。

其二，由于不同权重的梯度值差别巨大，因此选取一个全局的学习率很困难。 

Full Batch Learning 可以使用 **Rprop** 只基于梯度符号并且针对性单独更新各权值。

对于更大的数据集，以上 2 个好处又变成了 2 个坏处：

其一，随着数据集的海量增长和内存限制，一次性载入所有的数据进来变得越来越不可行。

其二，以 Rprop 的方式迭代，会由于各个 Batch 之间的采样差异性，各次梯度修正值相互抵消，无法修正。这才有了后来 **RMSProp** 的妥协方案。



既然 Full Batch Learning 并不适用大数据集，那么走向另一个极端怎么样？

所谓另一个极端，就是每次只训练一个样本，即 Batch_Size = 1。

这就是**在线学习（Online Learning）**。

线性神经元在均方误差代价函数的错误面是一个抛物面，横截面是椭圆。

对于多层神经元、非线性网络，在局部依然近似是抛物面。

使用在线学习，每次修正方向以各自样本的梯度方向修正，横冲直撞各自为政，**难以达到收敛**。**如图所示**：



可不可以选择一个适中的 Batch_Size 值呢？

当然可以，这就是**批梯度下降法（Mini-batches Learning）**。

因为如果数据集足够充分，那么用一半（**甚至少得多**）的数据训练算出来的梯度与用全部数据训练出来的梯度是**几乎一样**的。



在合理范围内，增大 Batch_Size 有何好处？

- 内存利用率提高了，大矩阵乘法的并行化效率提高。
- 跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。
- 在一定范围内，**一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。**



参考资料

1、深度学习中的batch的大小对学习效果有何影响？

https://www.zhihu.com/question/32673260

