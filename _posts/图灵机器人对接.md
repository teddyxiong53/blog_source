---
title: 图灵机器人对接
date: 2020-07-29 11:10:51
tags:
	- 产品
---

1

图灵机器人AI开发平台，是专门为儿童场景设计的。

可以应用的场景：

1、儿童机器人。

2、智能故事机。

3、儿童手表。

4、儿童平板。

5、公众号机器人。

6、绘本故事机。

7、绘本台灯。



api接入：这个不涉及语音输入，就是简单的文本性质的交互。

ai-wifi接入：这个就是语音输入，提到的目的url是不同的。这个的接口对api接入方便的有覆盖。只用这个就可以完成功能。



# 请求格式

请求的数据：

```
{
	"data": {
	
	},
	"key": "",
	"timestamp":""
}
```

data部分可以选择进行加密。

## data的构成

```
"data" : {
	"content": [
		{
			"data": "你好"
		}
	],
	"userInfo": {
		"uniqueId": "xxx"
	},
	"clientInfo": {
	
	}
}
```

只有userInfo是一定要有的。

### content的构成

```
type
	int类型，非必须。
	0：文本（默认）
	1：图片
	2：音频asr
	4：主动交互。
	5：音频技能（什么意思？）
data
	type=0，则data是文本的内容。
	type=1，则是图片的url。
	type=2或者5，则是数据的base64编码。
	type=4的，这个复杂一些，说明如下。
		"data": "osactive", # osgreet：开机提示语。osactive：主动交互。authorize：口语评测授权。
		"type": 4
```

### userInfo构成

```
uniqueId
	就是设备的sn号。长度小于等于32个字符。
	数字和字母构成。
	必须。
requestIP
	非必须。
```

### clientInfo构成

```
appState
	客户端状态。非必须。
robotSkill
	用户自定义参数。
```

# 响应格式

```
{
	"globalId": "xxx",
	"intent": {
		"code": 10002,
		"operateState": 1100,
	},
	"results": [
		{
			"groupType": 0,
			"values": {
				"text": "请告诉我您在哪个城市",
				"emotionId": 0
			},
			"resultType": "text"
		}
	]
}
```



# 儿童sdk

看这个主要是把流程梳理一下。

看这个就是使用了websocket的方式。

对应的demo编译安装不了。

算了。



# 对接过程

```
type=2-音频(asr)
	这个表示输入的音频，会进行语音分析。
type=5-音频(技能）
	这个表示就是一个命令，例如“打开台灯”这样的。
```

在之前对接讯飞的代码基础上，改动websocket地址。

可以连接上。

过了几秒收到这个消息：

```
{"code":4102,"globalId":"117837395253573002","message":"长时间未请求业务,关闭连接"}
```

请求的data构成

```
deviceId
	这个是sn号。16位的。以ai开头。我就这样：ai20200729000001
	必须
requestType
	int数组。0：asr。1：nlp。2：tts
	必须。
nlpRequest
	是一个json对象。
	有两种情况：一种是文本的形式，这个参考api文档里的格式。
	一种是图片或者音频这种二进制格式的。
	则里面的格式是这样：
	{
		"content": [
			{
				"data": "xxxxxxxxxxxxx", # 这个是32位的uuid。
				"type": 2  # 1：图片。2：音频asr。5：音频技能。
			}	
		]
	}
	必须。
asrRequest
	请求是音频asr的时候，需要的参数。
	json对象。里面的字段都是以asr开头的。
	{
		"asrFormatEnum": 0, # 0：pcm。1：opus。2：speex。
		"asrLanguageEnum": 0, # 0：中文。1：英文。
		"asrRateEnum": 1, # 0:8k。1:16k。
		#上面３个是必须的，下面的都不是必须的。
		
	}
binaryState
	json对象。
	二进制参数上传状态。
	openBinarysId：这个跟nlpRequest里的data的值一致。
	completeBinarysId
streamTts
	bool类型。
	默认是false。表示默认不是流式传输。
	不是流式，就是一个url，我们播放对应的url就好了。
	否则就是一段段的音频数据，我们播放不方便。
```

所以websocket传输音频是：先发一个openBinarysId，然后就是发送二进制音频数据，然后发送一个completeBinarysId。就好了。

关键就是nlpRequest，这个就相当于把api接入那里的文档，都相当于描述了这个json里的内容。



现在尝试发送语音数据。发送开始消息，得到这个返回。

```
{"code":4020,"globalId":"117846324311573002","message":"ASR权限异常"}
```

问了图灵的人，这个需要他们开一下权限，打开就好了。

现在发送openBinarysId，得到的返回：

```
{
	"asrResponse": {
		"binarysId": "32dc4e2edf58469ca60be3a0e4bf79e0",
		"state": 200,
		"value": ""
	},
	"code": 200,
	"globalId": "117906733283573001",
	"message": "success"
}
```

现在有个问题，就是很容易就出现服务端主动关闭websocket连接的情况。

我给nopoll注册了一个关闭回调函数。

我在这个回调函数里，调用disconnect和connect函数，尝试进行重连。

当连接被断开时，nopoll_loop_wait这个函数一直卡住了。

所以我只能在收到消息后，自己主动断开，过几秒再重新连接。

OS_ThreadDelete 关键是这个函数卡住了。

关键还是nopoll_loop_wait这个函数卡住导致的。

换成nopoll_conn_get_msg也一样会卡住。

所以问题就是，服务端关闭连接后，设备端的nopoll都会卡住。

这个怎么解决？

在close回调里，只进行打印，然后启动定时器来做断开和连接的操作。

这样可以退出循环。

但是系统卡死了。

```
OS_ThreadDelete(&this->recv_thread);
```

线程delete是在线程函数结尾的地方调用。

系统卡死是因为我启动一个timer导致的。

为什么？

启动定时器有什么问题？

我在启动定时器后面加上一行打印，再打开编译试一下，又好了。真是无语。

但是连接会失败。

是不是nopoll_conn还是需要重新创建呢？

创建了。可以连接成功。

反复被断开再连接，没有内存泄漏。

现在问题是图灵服务端在语音输入时，并没有下发云端vad。

在20s后，直接报错提示交互时间太长。

使用云端vad，需要设置下面这3个参数。

![1596163276185](../images/random_name/1596163276185.png)

现在可以收到vad了。

```
{"code":220,"globalId":"117993116306599001","message":"参数上传完成，正在请求nlp/tts"}
```

```
{
	"code": 200,
	"globalId": "117993116306599001",
	"message": "success",
	"nlpResponse": {
		"intent": {
			"code": 100000,
			"operateState": 1010
		},
		"results": [{
			"groupType": 0,
			"resultType": "text",
			"values": {
				"emotionId": 0,
				"sentenceId": 0,
				"text": "蛙！图灵机器人知道你的年纪了哦～"
			}
		}]
	}
}
```

我也没有发送什么，服务端返回了这个。

```
{"code":4008,"globalId":"117997982585573001","message":"数据内容格式错误"}
```

是因为我有个地方发送了`__END__`导致的。

但是websocket的方式，并不完善。

所以还是改成http方式来做请求，可以从全志拿到XR872的本地VAD算法库。

# http对接

地址是：

http://smartdevice.ai.tuling123.com/speech/chat
Method: POST
Content-Type: multipart/form-data



speech为音频文件，当为主动交互请求（type=1）或者提示语请求（type=2）时，该字段无效。

设备ID(deviceId)由设备端自行生成，共16位(只支持数字和字母组合，其他字符不支持)，需要保证每台设备的deviceId唯一不变，建议以 "ai" 字符开头，如ai11223344556677。

首次请求token值可以为空，之后的请求分为流式识别和非流式识别两种情况。

目前是10分钟的有效期，后期可能会根据业务需要随时变动。只要开发者遵循一个原则“每次请求token值使用上次请求返回的token值”，token有效期对接口调用无任何影响。

另外，开发者不用担心token过期问题，本接口为提升用户体验，已经做token容错处理，即每个token有效周期内允许接口调用过程中允许5次token错误请求，并在该5次请求中都会返回正确的token用于客户端更新本地token，即已经做了容错处理。所以不用担心当程序异常退出之后，token过期造成服务无法正常使用问题。

语音识别的接口是：

http://smartdevice.ai.tuling123.com/speech/v2/asr



先用文本合成来验证通信。

http://smartdevice.ai.tuling123.com/speech/v2/tts



```
First, initialize the AES context with your key, and then encrypt the data (with padding) to the output buffer with your iv:
```

uid必须用aes加密。

我看芯片是有硬件aes加密的。

现在就是要把二者匹配起来。

```
HAL_Status HAL_AES_Encrypt(CE_AES_Config *aes, uint8_t *plain, uint8_t *cipher, uint32_t size)
```

除了config，还有3个参数，plain表示加密之前的数据，应该填入uid。

cipher，应该给一个buffer，用来接收加密后的数据。

size，是加密前数据的长度。字节数。

对于图灵的算法。

```
void AES128_CBC_encrypt_buffer(uint8_t* output, uint8_t* input, uint32_t length, const uint8_t* key, const uint8_t* iv);
output：一个buffer。
input：ai20200729000001 这个id
length：input的长度。16字节。
key：是api secret。
iv：是api key。
```

算了。我先直接用算法。后面再优化。

不用xr872里的http了。很不好用。



把rt-thread里的webclient移植过来用。

现在可以了。

webclient感觉好用一些，代码只有一个c文件和一个头文件。

添加header的方式也更方便一些。

不过前面有问题，关键不是http库，有两点：

1、uid没有加密。这个改了后，就可以返回带错误码的提示信息，而不是只是system error。现在是提示apikey类型不对。

2、产品需要配置为wifi-ai模式。改了后，就是提示：

```
{"code":29999,"token":"2af62825b9cd49568cc55a6256a86239","func":{"error":"您还没有设置开机提示语"}}
```

这样通信就是正常的。

现在把代码整理到板端。

完全相同的数据，在板端就是错误。

webclient_read_line 这个函数返回了负数。

抓包表现就是没有返回值。

换个路由器试一下。还是一样。

换httpbin测试一下。

也还是一样。

再改回xr872自带的http库试一下。

这个是返回400错误。

放弃webclient，还是把xr872自带的调试一下。

仔细看抓的包，发现板端的header和body是分开发送的。

而服务端在收到header之后，就回复了400错误了。

然后板端才发送的body。



仔细看了一下webclient的，发现这个在板端跑起来有问题，是因为设置recv超时有问题。

给的是一个timeval，但是lwip下面的实现，是当成ms来用。

肯定就不对了。

改一下。webclient工作就正常了。

后续就用webclient来做了。



webclient还是不够完善，例如很明显没有考虑post二进制数据的情况。

不过这个点也很容易改。

总的来看，代码足够简单，风格也好，就看能不能稳定运行了。

按道理应该可以。



现在把post过程需要的内存梳理一下，尽量减少不必要的内存分配和memset操作。

header给1024字节。

post的回复，给1024字节。

这2个应该是够的。

post_data部分：这个目前先给4096字节。

有这3个缓冲区够了吧。



然后是看socket的持久化，不用每次都重新连接。



不要支持使用webclient提供的post函数，

自己在外面自己用函数封装一下。



现在尽量不要自己调用close socket函数。

过了一会儿，就无法通信了。感觉服务端关闭socket也挺快的。



语音数据编码。

官方推荐opus。

```
当asr=4(推荐)时：opus；
```

因为本地vad的限制，那么我一个语音片段就定为640字节。

把实时录音进行编码。

得到的文件，并不能进行播放。

opus文件是需要加ogg头部才能被正常播放的。

看opus的库，里面的例子，也没用写文件，只是把一个pcm文件，先编码，再解码。

所以我在板端也可以这样来验证。

可以编码再解码出来。



websocket方式也可以用opus。因为我这个通路的可以通。所以用这个通路测试一下opus是否可以正常被服务端解析。

但是打开opus编码的，内存不够了。

算了，不试了。



目前看到的情况有这些特点：

1、编码640字节的音频数据，编码后是50字节左右。压缩比在12左右。对应的时间是20ms。

2、我觉得可以用一个可以存放2s音频数据的队列。因为2s的数据，也就是100个包。占用内存不过5K左右。

3、存2s的数据，一次性发送，这样性价比才高。因为每次发送有几百字是无效的，如果一次发送几十个字节的音频，大部分是无效数据，性价比太低了。

4、队列满，或者vad结束，都触发网络发送。

这个过程有几次拷贝，先不管，先保证功能实现，再看优化吧。



现在要把http过程整理了一下。

发送时，如果发现当前是断开的，要马上进行连接。

断开状态要及时通报。

这个就要用信号量机制来做了。



对端socket已经关闭了。本机并不知道，一直要到read返回为0才知道。

这个时候已经晚了。

先停止这些http socket优化。

反复连接就反复连接。一次多发送一点数据就好了。

不必要的header，都去掉，例如keep-alive，就没有必要了。

不用keep-alive了，那么也就不需要content-length了。

现在把图灵的台灯抓包分析一下。

一开机，就是向39.96.131.218这个服务器发送type为5的文本请求，内容是“播放儿歌“。

向39.106.240.164，post了一个json数据，看内容是请求授权。

语音交互的时候，每次发送的语音数据也是在1300字节左右，时间大概是1s左右。

是opus编码。

我问的是”现在几点钟“。总共三个包，index为1,2，-3

前后时间差是1.3秒左右。

最后一个应该是靠vad来做。

speech这个字段里，filename是写的”speech.opus“。



现在可以把音频数据发送到服务器了。

```
memcpy(input+text_len+audio_size,NEWLINE, 2);
text_len += 2;//加上audio data后面的一个换行。
memcpy(input+text_len+audio_size, gen_end_boundary(), END_LEN);
memcpy(input+text_len+audio_size+END_LEN, NEWLINE, 2);
text_len += 2;
```

这2个换行是必须的，不然服务端就返回chunk的http头。

导致错误。

先看编码解码测试的。

为什么解码后，数据少了一半呢？

```
[xhl][DEBUG][test_encode_decode][81][3252]: actual read len:640
[xhl][DEBUG][test_encode_decode][92][3261]: encode_len:54
[xhl][DEBUG][test_encode_decode][96][3263]: decode len:320
```

因为解码函数的返回值是sample个数。我写入的时候乘以2就好了。

把得到的文件，用audacity打开看，可以看到编码解码后的文件，波形跟原始文件基本一样。

所以可以确认编码解码没有问题。

测试了一下发送的时间，基本上在500ms左右。一个编码的数据，在50字节左右。

现在内存也所剩不多。只剩下20K左右。

现在总共可供分配的就是230K左右。

opus对空间的消耗非常大。

现在只把确定的input内容生成过程同步来做。这里面就是一些拷贝，json生成，做的事情不多。

但是就是这样，都需要40ms。

可见这个芯片的处理能力真的很弱。



其实当前webclient帮我做的事情并不多。

我可以把http，完成用tcp的方式来编程。

因为所有数据都是我手动拼出来的。

返回内容也是非常确定的，就是json。所以回复的解析都可以非常简单，直接找大括号就好了。

socket就用阻塞模式。

我就2个线程，一个send，一个recv。当recv返回0的时候，就说明socket被服务端断掉了。

然后我就可以重新再建立socket。

可以在Linux上测试一下。



先看opus编码数据不能识别的问题。

发现是每次得到的编码数据，发送之前，还需要加上8个字节的头部。

4个字节的长度，4个字节的enc_final_range。这个具体是什么不清楚。但是有接口可以get出来。

先试一下。

加上就可以了。



现在内存确实太紧张了。

看看opus用了多少内存。

使用了opus之后，内存还剩20K。不用opus，内存还剩200K。

amr压缩比，从网上看是非常高的。



发现opus的，没有明确写到ld脚本里。

加入再看看内存使用情况。

加入后，现在内存还剩下180K。还好。



从抓包图灵的其他机器来看，使用的就是http。所以我手动通过socket方式来做，存在可行性。

如果是https，那我就没招了。只能用现成的库。



接收的关键是先拿到这个长度。

```
Content-Length: 9593
```



webclient，采取先收取一行的方法，具体就是一个字节一个字节地收。

这个稳妥，但是效率低了点。



发送的时候，header和body要分开吗？

一起发送吧。虽然生成上会麻烦一些。

会有问题。

连接会被服务端reset掉。

放弃自己写的http。

还是在webclient上改。



在Linux试了一下我自己写的http库连接图灵的，没有问题。

所以再次尝试一下放到板端运行看看。

可以的。

所以还是把我的http库放进来用。我的库可以有效利用长连接。

我的http库可以正常工作了。

现在看返回的json。里面有好几个url。

```
{
	"code": 20007,
	"asr": "播放音乐",
	"tts": "好的，我要开始唱摇啊摇这首歌啦",
	"nlp": ["http://turing-iot.oss-cn-beijing.aliyuncs.com/tts/tts-a1d84bcae1c848f892bcfe0effe9d5e8-777815ef3611488db61b7a7b915b76d9.mp3"],
	"token": "7f542847a5224164bc5f1cfb242bc7e1",
	"func": {
		"duration": 0,
		"operate": 1000,
		"singer": "",
		"originalTitle": "",
		"originalSinger": "",
		"isPlay": 1,
		"tip": "http://turing-iot.oss-cn-beijing.aliyuncs.com/tts/tts-a1d84bcae1c848f892bcfe0effe9d5e8-1bfaaa155cf04aaa931e07540d0d7fe5.mp3",
		"id": 109201,
		"title": "摇啊摇",
		"url": "http://iot-cdn.tuling123.com/202008081311/f5573ed2a69833b126adee1e0c92d52d/media/audio/20180524/a3f6e3c80f0e498c8a83b019fba9d3e9.mp3"
	},
	"emotion": 0
}
```

有3个：

1、nlp。这个是tts的语音播报。

2、tip。这个是播放之前报一下歌名。”摇啊摇“。

3、url。这个就是歌曲的url。

所以这3个音频，要依次播放。

都用urlplayer来播放。

所以UrlPlayer需要加一个队列。

另外有个问题，就是歌曲的url。因为长度不确定。

给固定的 数组，实在有点浪费。

所以还是用动态分配吧。注意处理好。避免内存泄漏。

所以还是需要在playerwrapper里，放一个当前播放的url。

所有的url，都用动态分配的方式，在播放器回调函数里，进行url的释放操作。



现在是收到了中间的结果，也进行处理了。

```
b'[D][_play][20][26078]: play `\xdb"'
[D][_play][22][26078]: ---------strdup ptr:0x21d5f0--------
b'[PLAYER_INFO] request to play : `\xdb"'
xplayer thread dealing with msg.messageId: 0x101
[D][_player_callback][44][26094]: media is prepared. play media now.
xplayer thread dealing with msg.messageId: 0x105
[PLAYER_ERR] start() return fail.
```

清空一下buffer。

现在3个文件，播放前面2个都是失败的。

每次都是如此。为什么？

前面2个文件都是在阿里云上的。

```
ERROR : httpStream : send error.
ERROR : httpStream : xxx CdxHttpSendRequest failed.
ERROR : httpStream : Start http stream failed.
ERROR : awplayer : open stream fail, uri(http://turing-iot.oss-cn-beijing.aliyuncs.com/tts/tts-a1d84bcae1c848f892bcfe0effe9d5e8-024ea4c1ed32451990f8779dbd3afa10.mp3)
```

这个链接没有问题，在电脑上可以播放。

出错的时候，再尝试几次看看。

知道原因了，因为我判断正在播放不对，playing、prepareing，prepared，应该这三种都算播放中的。

把开机提示音加上。



看看成语接龙的具体过程。

接龙采用积分的形式，回答错误会继续进行，可以获取提示，成语有三次跳过的机会，三次跳过后会结束游戏并公布分值。此技能为多轮技能，记得退出哦。

用户先说：进入成语接龙。

云端返回：20000 。提示已经进入成语接龙。而且给出第一个成语。

设备应该进入listening状态。等待用户输入。

在vad结束后，把当前的语音上传。

用户的输入可以是：一个成语，提示，退出。



有很多的技能返回的code都是20000。



在录音循环里，20ms一次循环，大概耗时情况是这样。vad基本不耗时。可以忽略。

编码时间7ms，太长了。

拷贝时间也太长了3ms。



```
[D][write_data_to_wakeup_lib][163][15521]: vad use time:0
[D][write_data_to_wakeup_lib][178][15528]: encode use time:7
[D][write_data_to_wakeup_lib][203][15528]: copy use time:3
[D][write_data_to_wakeup_lib][215][15625]: send use time:96
```

我把打印字符数减少，再统计一下。

```
vad:0
enc:6
cpy:0
snd:87
```

vad和拷贝的时间可以忽略。

编码时间还是长了。

编码640个字节的数据，就需要6ms。

用2次循环的数据，一起进行一次编码看看。

先优化send这里吧。

json生成，只用了1ms。

send_event里，可以减少一次拷贝。



post内容长度计算：

前面字符：这个需要先生成json。再strlen计算一下。可以先不拷贝。

中间二进制音频：这个是参数传递进来，长度明确。

后面的字符：这个长度固定。

也先不优化，这个调整后，代码不如现在清晰。

这里不是问题的关键。

先减少不需要的打印。

去掉打印后，send_event的统计时间就变成9ms左右了。这个点不用优化了。



语音上传这块没有什么问题。

现在重点还是看交互逻辑。



也不是，语音识别，感觉还是丢了一些内容。

例如短的输入，很难被识别到，例如：答案。



有出现这种错误。长度计算出错了。

```
[D][_trigger][216][533531]: try to send bytes:1224768960
```



主动交互是图灵提供的个性化能力，当机器人在联网下的静默状态（如用户长时间没有和机器人进行交互）时，由机器人主动发起交互，可以在【“AI开放平台” - “机器人功能” - “主动交互语”】处设置相关交互语句，当达到条件后请求图灵主动交互接口时可返回设置的相应内容。

是这种：

```
"小朋友你好，我们好久没聊天了哦，我想你了呢。",
```

这种情况我先不管。



现在要把多轮做起来。

当前要做还是很简单的。我只要让vad持续执行就好了。

现在有出现这种错误。

```
{"code":43038,"err":"index 1 has fail","token":"a8f70615d2874b45abffd2f645412e21"}
```

如果是出错的，需要让交互状态回到idle。

```
--------------thinking---------------
[D][_recv_thread_proc][179][133171]: recv len:258, errno:0
[D][_recv_thread_proc][200][133171]: json:{"code":40020,"err":"speech is null"}
```

错误还需要区分处理一下。



就放着，有出现这样的情况。

```
[UMAC WARN] net80211_linkoutput():211, ifp->if_transmit() failed, err 61
[UMAC WARN] net80211_linkoutput():211, ifp->if_transmit() failed, err 61
```

这个代码都没有开放的。

是在libnet80211.a里。



对于这种错误：

```
{"code":43038,"err":"index 1 has fail","token":"a8f70615d2874b45abffd2f645412e21"}
```

当前无法恢复，我估计应该是需要换一个g_identify

我当前只在命令触发的地方才更新了g_identify。所以需要每次触发交互都换一下就好了。



现在放了一段时间，因为vad的频繁触发，现在没有死机，但是发送命令，提示

```
no buf for rx, discard received data
```

我觉得可以加上一个超时，超过一段时间，需要继续交互，就需要按键触发一下。

这样可以有效避免在长期限制的这种vad频繁自动触发问题。

有时候收到回复特别慢，过了20秒才收到。而且这个时间内没有断开。



关于连续交互，我觉得可能还是要分场景来出处理，有时候，就不要连续交互了。

根据返回的code来决定。是否要继续保持唤醒状态。



从日志看，当前的确在泄漏内存。

```
	Line 1954: 200 heap total 360464, use 224072, free 136392, [0x20fbf0, 0x246738, 0x267c00)
	Line 6158: 200 heap total 360464, use 276476, free 83988, [0x20fbf0, 0x2533ec, 0x267c00)
```

分析这之间的日志。

有出现一次这个错误。其他的感觉没有什么异常。

```
json:{"code":40002,"err":"uid illegal value",
```

uid出错，不应该啊。

因为uid只是开头计算一次，后面都是一直使用同一个，为什么会突然出现一个uid错误呢？

在播放器出错的时候，内存有1284字节被占用没有释放。

```
ERROR : httpStream : CdxStreamOpen failed. 'tcp://turing-iot.oss-cn-beijing.aliyuncs.com:80'
ERROR : httpStream : xxx CdxHttpSendRequest failed.
ERROR : httpStream : Start http stream failed.
ERROR : awplayer : open stream fail, uri(http://turing-iot.oss-cn-beijing.aliyuncs.com/tts/tts-a1d84bcae1c848f892bcfe0effe9d5e8-be6ad52788534fd3b393be09992e84c9.mp3)
ERROR : demuxComponent : DEMUX_ERROR_IO
[PLAYER_WRN] open media source fail.
reason: maybe the network is bad, or the mus[XRADIO_INTERNAL_CODEC] Rx : overrun and stop dma rx...
ic file is not good.
[PLAYER_INFO] cedarx cb complete.
[PLAYER_INFO] event: 9
xplayer thread dealing with msg.messageId: 0x108
WARNING:invalid stop operation, player already in stopped status.
[D][_player_callback][57][316816]: error occur
[D][_player_callback][89][316820]: ----------strdup free:0x21d610----------
[D][write_data_to_wakeup_lib][181][316827]: -----------use heap:-1284---------
```

现在是网络不通了。

这个时候，我先ping smartdevice.ai.tuling123.com

得到402错误。

再ping  114.114.114.114

程序死机。

LR也变成0 了。

打印的任务信息也变成这样了。

![1597041572513](../images/random_name/1597041572513.png)



把mic的灵敏度提高看看，能不能提高语音识别的准确率。

当前是3，改成7，导致vad一直是检测活跃状态。

最终是靠30个语音片段的调节才进入thinking状态。

灵敏度改成5看看。还行，先用5吧。

改成4吧。

现在cpu的频率是240M。

```
cpu clock 240000000 Hz
ahb1 clock 240000000 Hz
ahb2 clock 120000000 Hz
apb clock 40000000 Hz
dev clock 192000000 Hz
apbs clock 48000000 Hz
dev2 clock 384000000 Hz
HF clock 40000000 Hz
LF clock 35950 Hz
sdk option:
XIP : enable
PSRAM : enable
INT LF OSC : enable
```

而代码里有个这样的配置。

打开看看。

```
__CONFIG_CPU_SUPPORT_349MHZ
```

打开后，开机打印的cpu频率还是240M。

录音的overrun还是有。

手册里不是说cpu频率可以到384M吗？

为什么打印出来是240M呢？

下载了最新的xr872的sdk，也是一样的240M。

现在每次取10ms的数据看看。

这个不太好。

看看vad头文件里，有这样的注释。

我当前是16K，那么也是可以支持30ms的数据了？试一下。

```
// Checks for valid combinations of |rate| and |frame_length|. We support 10,
// 20 and 30 ms frames and the rates 8000, 16000 and 32000 Hz.
```

30ms的话，opus编码又不支持了。

所以还是640才能都满足。

把vad的4个mode，0到3，都试一下。

```
0：Quality mode.
	这个在环境噪音下，一直是激活状态。
1：Low bitrate mode.
	这个虽然容易被触发，但是也很快就变成非活跃状态。
2：Aggressive mode.
	
3：Very aggressive mode.
	我之前一直用的3 。
```

aggressive mode可以翻译为积极模式。

可以理解为敏感程度。那么3就相当于比2要敏感。

1和2的敏感程度区别是什么？





# 本地vad

全志给了一个基于webrtc的vad检测库。

试了一下，16bit，单声道，如果数据buffer为1280字节，则处理函数报错。640则正常。320也正常。

处理函数不是阻塞的。

环境噪音也会触发vad识别。这个确实比较难以界定。

只有一个参数可以调，就是set_mode，可以设置0到3



# 配网方式

支持三种配网方式。

蓝牙、声波、



# mqtt

MQTT数据通道主要用于图灵IOT云端服务向设备端推送消息，

包括家长端H5资源页音频等消息的推送、微信公众号语音/文字信息推送、绑定解绑消息状态推送、设备状态上报等HTTP接口请求数据的推送等。

目前支持三种类型消息：音乐，故事，对讲。

建议设备联网成功后，立即建立与MQTT服务器的连接，以防推送的消息丢失。

**如果有使用mqtt心跳的话，建议1分钟请求一次（不要太频繁），连续三到五次没有收到回复断开链接后重新连接。**



参考资料

1、官网

http://docs.turingos.cn/