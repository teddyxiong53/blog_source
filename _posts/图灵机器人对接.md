---
title: 图灵机器人对接
date: 2020-07-29 11:10:51
tags:
	- 产品
---

1

图灵机器人AI开发平台，是专门为儿童场景设计的。

可以应用的场景：

1、儿童机器人。

2、智能故事机。

3、儿童手表。

4、儿童平板。

5、公众号机器人。

6、绘本故事机。

7、绘本台灯。



api接入：这个不涉及语音输入，就是简单的文本性质的交互。

ai-wifi接入：这个就是语音输入，提到的目的url是不同的。这个的接口对api接入方便的有覆盖。只用这个就可以完成功能。



# 请求格式

请求的数据：

```
{
	"data": {
	
	},
	"key": "",
	"timestamp":""
}
```

data部分可以选择进行加密。

## data的构成

```
"data" : {
	"content": [
		{
			"data": "你好"
		}
	],
	"userInfo": {
		"uniqueId": "xxx"
	},
	"clientInfo": {
	
	}
}
```

只有userInfo是一定要有的。

### content的构成

```
type
	int类型，非必须。
	0：文本（默认）
	1：图片
	2：音频asr
	4：主动交互。
	5：音频技能（什么意思？）
data
	type=0，则data是文本的内容。
	type=1，则是图片的url。
	type=2或者5，则是数据的base64编码。
	type=4的，这个复杂一些，说明如下。
		"data": "osactive", # osgreet：开机提示语。osactive：主动交互。authorize：口语评测授权。
		"type": 4
```

### userInfo构成

```
uniqueId
	就是设备的sn号。长度小于等于32个字符。
	数字和字母构成。
	必须。
requestIP
	非必须。
```

### clientInfo构成

```
appState
	客户端状态。非必须。
robotSkill
	用户自定义参数。
```

# 响应格式

```
{
	"globalId": "xxx",
	"intent": {
		"code": 10002,
		"operateState": 1100,
	},
	"results": [
		{
			"groupType": 0,
			"values": {
				"text": "请告诉我您在哪个城市",
				"emotionId": 0
			},
			"resultType": "text"
		}
	]
}
```



# 儿童sdk

看这个主要是把流程梳理一下。

看这个就是使用了websocket的方式。

对应的demo编译安装不了。

算了。



# 对接过程

```
type=2-音频(asr)
	这个表示输入的音频，会进行语音分析。
type=5-音频(技能）
	这个表示就是一个命令，例如“打开台灯”这样的。
```

在之前对接讯飞的代码基础上，改动websocket地址。

可以连接上。

过了几秒收到这个消息：

```
{"code":4102,"globalId":"117837395253573002","message":"长时间未请求业务,关闭连接"}
```

请求的data构成

```
deviceId
	这个是sn号。16位的。以ai开头。我就这样：ai20200729000001
	必须
requestType
	int数组。0：asr。1：nlp。2：tts
	必须。
nlpRequest
	是一个json对象。
	有两种情况：一种是文本的形式，这个参考api文档里的格式。
	一种是图片或者音频这种二进制格式的。
	则里面的格式是这样：
	{
		"content": [
			{
				"data": "xxxxxxxxxxxxx", # 这个是32位的uuid。
				"type": 2  # 1：图片。2：音频asr。5：音频技能。
			}	
		]
	}
	必须。
asrRequest
	请求是音频asr的时候，需要的参数。
	json对象。里面的字段都是以asr开头的。
	{
		"asrFormatEnum": 0, # 0：pcm。1：opus。2：speex。
		"asrLanguageEnum": 0, # 0：中文。1：英文。
		"asrRateEnum": 1, # 0:8k。1:16k。
		#上面３个是必须的，下面的都不是必须的。
		
	}
binaryState
	json对象。
	二进制参数上传状态。
	openBinarysId：这个跟nlpRequest里的data的值一致。
	completeBinarysId
streamTts
	bool类型。
	默认是false。表示默认不是流式传输。
	不是流式，就是一个url，我们播放对应的url就好了。
	否则就是一段段的音频数据，我们播放不方便。
```

所以websocket传输音频是：先发一个openBinarysId，然后就是发送二进制音频数据，然后发送一个completeBinarysId。就好了。

关键就是nlpRequest，这个就相当于把api接入那里的文档，都相当于描述了这个json里的内容。



现在尝试发送语音数据。发送开始消息，得到这个返回。

```
{"code":4020,"globalId":"117846324311573002","message":"ASR权限异常"}
```

问了图灵的人，这个需要他们开一下权限，打开就好了。

现在发送openBinarysId，得到的返回：

```
{
	"asrResponse": {
		"binarysId": "32dc4e2edf58469ca60be3a0e4bf79e0",
		"state": 200,
		"value": ""
	},
	"code": 200,
	"globalId": "117906733283573001",
	"message": "success"
}
```

现在有个问题，就是很容易就出现服务端主动关闭websocket连接的情况。

我给nopoll注册了一个关闭回调函数。

我在这个回调函数里，调用disconnect和connect函数，尝试进行重连。

当连接被断开时，nopoll_loop_wait这个函数一直卡住了。

所以我只能在收到消息后，自己主动断开，过几秒再重新连接。

OS_ThreadDelete 关键是这个函数卡住了。

关键还是nopoll_loop_wait这个函数卡住导致的。

换成nopoll_conn_get_msg也一样会卡住。

所以问题就是，服务端关闭连接后，设备端的nopoll都会卡住。

这个怎么解决？

在close回调里，只进行打印，然后启动定时器来做断开和连接的操作。

这样可以退出循环。

但是系统卡死了。

```
OS_ThreadDelete(&this->recv_thread);
```

线程delete是在线程函数结尾的地方调用。

系统卡死是因为我启动一个timer导致的。

为什么？

启动定时器有什么问题？

我在启动定时器后面加上一行打印，再打开编译试一下，又好了。真是无语。

但是连接会失败。

是不是nopoll_conn还是需要重新创建呢？

创建了。可以连接成功。

反复被断开再连接，没有内存泄漏。

现在问题是图灵服务端在语音输入时，并没有下发云端vad。

在20s后，直接报错提示交互时间太长。

使用云端vad，需要设置下面这3个参数。

![1596163276185](../images/random_name/1596163276185.png)

现在可以收到vad了。

```
{"code":220,"globalId":"117993116306599001","message":"参数上传完成，正在请求nlp/tts"}
```

```
{
	"code": 200,
	"globalId": "117993116306599001",
	"message": "success",
	"nlpResponse": {
		"intent": {
			"code": 100000,
			"operateState": 1010
		},
		"results": [{
			"groupType": 0,
			"resultType": "text",
			"values": {
				"emotionId": 0,
				"sentenceId": 0,
				"text": "蛙！图灵机器人知道你的年纪了哦～"
			}
		}]
	}
}
```

我也没有发送什么，服务端返回了这个。

```
{"code":4008,"globalId":"117997982585573001","message":"数据内容格式错误"}
```

是因为我有个地方发送了`__END__`导致的。

但是websocket的方式，并不完善。

所以还是改成http方式来做请求，可以从全志拿到XR872的本地VAD算法库。

# http对接

地址是：

http://smartdevice.ai.tuling123.com/speech/chat
Method: POST
Content-Type: multipart/form-data



speech为音频文件，当为主动交互请求（type=1）或者提示语请求（type=2）时，该字段无效。

设备ID(deviceId)由设备端自行生成，共16位(只支持数字和字母组合，其他字符不支持)，需要保证每台设备的deviceId唯一不变，建议以 "ai" 字符开头，如ai11223344556677。

首次请求token值可以为空，之后的请求分为流式识别和非流式识别两种情况。

目前是10分钟的有效期，后期可能会根据业务需要随时变动。只要开发者遵循一个原则“每次请求token值使用上次请求返回的token值”，token有效期对接口调用无任何影响。

另外，开发者不用担心token过期问题，本接口为提升用户体验，已经做token容错处理，即每个token有效周期内允许接口调用过程中允许5次token错误请求，并在该5次请求中都会返回正确的token用于客户端更新本地token，即已经做了容错处理。所以不用担心当程序异常退出之后，token过期造成服务无法正常使用问题。

语音识别的接口是：

http://smartdevice.ai.tuling123.com/speech/v2/asr



先用文本合成来验证通信。

http://smartdevice.ai.tuling123.com/speech/v2/tts



```
First, initialize the AES context with your key, and then encrypt the data (with padding) to the output buffer with your iv:
```

uid必须用aes加密。

我看芯片是有硬件aes加密的。

现在就是要把二者匹配起来。

```
HAL_Status HAL_AES_Encrypt(CE_AES_Config *aes, uint8_t *plain, uint8_t *cipher, uint32_t size)
```

除了config，还有3个参数，plain表示加密之前的数据，应该填入uid。

cipher，应该给一个buffer，用来接收加密后的数据。

size，是加密前数据的长度。字节数。

对于图灵的算法。

```
void AES128_CBC_encrypt_buffer(uint8_t* output, uint8_t* input, uint32_t length, const uint8_t* key, const uint8_t* iv);
output：一个buffer。
input：ai20200729000001 这个id
length：input的长度。16字节。
key：是api secret。
iv：是api key。
```

算了。我先直接用算法。后面再优化。

不用xr872里的http了。很不好用。



把rt-thread里的webclient移植过来用。

现在可以了。

webclient感觉好用一些，代码只有一个c文件和一个头文件。

添加header的方式也更方便一些。

不过前面有问题，关键不是http库，有两点：

1、uid没有加密。这个改了后，就可以返回带错误码的提示信息，而不是只是system error。现在是提示apikey类型不对。

2、产品需要配置为wifi-ai模式。改了后，就是提示：

```
{"code":29999,"token":"2af62825b9cd49568cc55a6256a86239","func":{"error":"您还没有设置开机提示语"}}
```

这样通信就是正常的。

现在把代码整理到板端。

完全相同的数据，在板端就是错误。

webclient_read_line 这个函数返回了负数。

抓包表现就是没有返回值。

换个路由器试一下。还是一样。

换httpbin测试一下。

也还是一样。

再改回xr872自带的http库试一下。

这个是返回400错误。

放弃webclient，还是把xr872自带的调试一下。

仔细看抓的包，发现板端的header和body是分开发送的。

而服务端在收到header之后，就回复了400错误了。

然后板端才发送的body。



仔细看了一下webclient的，发现这个在板端跑起来有问题，是因为设置recv超时有问题。

给的是一个timeval，但是lwip下面的实现，是当成ms来用。

肯定就不对了。

改一下。webclient工作就正常了。

后续就用webclient来做了。



webclient还是不够完善，例如很明显没有考虑post二进制数据的情况。

不过这个点也很容易改。

总的来看，代码足够简单，风格也好，就看能不能稳定运行了。

按道理应该可以。



现在把post过程需要的内存梳理一下，尽量减少不必要的内存分配和memset操作。

header给1024字节。

post的回复，给1024字节。

这2个应该是够的。

post_data部分：这个目前先给4096字节。

有这3个缓冲区够了吧。



然后是看socket的持久化，不用每次都重新连接。



不要支持使用webclient提供的post函数，

自己在外面自己用函数封装一下。



现在尽量不要自己调用close socket函数。

过了一会儿，就无法通信了。感觉服务端关闭socket也挺快的。



语音数据编码。

官方推荐opus。

```
当asr=4(推荐)时：opus；
```

因为本地vad的限制，那么我一个语音片段就定为640字节。

把实时录音进行编码。

得到的文件，并不能进行播放。

opus文件是需要加ogg头部才能被正常播放的。

看opus的库，里面的例子，也没用写文件，只是把一个pcm文件，先编码，再解码。

所以我在板端也可以这样来验证。

可以编码再解码出来。



websocket方式也可以用opus。因为我这个通路的可以通。所以用这个通路测试一下opus是否可以正常被服务端解析。

但是打开opus编码的，内存不够了。

算了，不试了。



目前看到的情况有这些特点：

1、编码640字节的音频数据，编码后是50字节左右。压缩比在12左右。对应的时间是20ms。

2、我觉得可以用一个可以存放2s音频数据的队列。因为2s的数据，也就是100个包。占用内存不过5K左右。

3、存2s的数据，一次性发送，这样性价比才高。因为每次发送有几百字是无效的，如果一次发送几十个字节的音频，大部分是无效数据，性价比太低了。

4、队列满，或者vad结束，都触发网络发送。

这个过程有几次拷贝，先不管，先保证功能实现，再看优化吧。



现在要把http过程整理了一下。

发送时，如果发现当前是断开的，要马上进行连接。

断开状态要及时通报。

这个就要用信号量机制来做了。



对端socket已经关闭了。本机并不知道，一直要到read返回为0才知道。

这个时候已经晚了。

先停止这些http socket优化。

反复连接就反复连接。一次多发送一点数据就好了。

不必要的header，都去掉，例如keep-alive，就没有必要了。

不用keep-alive了，那么也就不需要content-length了。

现在把图灵的台灯抓包分析一下。

一开机，就是向39.96.131.218这个服务器发送type为5的文本请求，内容是“播放儿歌“。

向39.106.240.164，post了一个json数据，看内容是请求授权。

语音交互的时候，每次发送的语音数据也是在1300字节左右，时间大概是1s左右。

是opus编码。

我问的是”现在几点钟“。总共三个包，index为1,2，-3

前后时间差是1.3秒左右。

最后一个应该是靠vad来做。

speech这个字段里，filename是写的”speech.opus“。



现在可以把音频数据发送到服务器了。

```
memcpy(input+text_len+audio_size,NEWLINE, 2);
text_len += 2;//加上audio data后面的一个换行。
memcpy(input+text_len+audio_size, gen_end_boundary(), END_LEN);
memcpy(input+text_len+audio_size+END_LEN, NEWLINE, 2);
text_len += 2;
```

这2个换行是必须的，不然服务端就返回chunk的http头。

导致错误。

先看编码解码测试的。

为什么解码后，数据少了一半呢？

```
[xhl][DEBUG][test_encode_decode][81][3252]: actual read len:640
[xhl][DEBUG][test_encode_decode][92][3261]: encode_len:54
[xhl][DEBUG][test_encode_decode][96][3263]: decode len:320
```

因为解码函数的返回值是sample个数。我写入的时候乘以2就好了。

把得到的文件，用audacity打开看，可以看到编码解码后的文件，波形跟原始文件基本一样。

所以可以确认编码解码没有问题。

测试了一下发送的时间，基本上在500ms左右。一个编码的数据，在50字节左右。

现在内存也所剩不多。只剩下20K左右。

现在总共可供分配的就是230K左右。

opus对空间的消耗非常大。

现在只把确定的input内容生成过程同步来做。这里面就是一些拷贝，json生成，做的事情不多。

但是就是这样，都需要40ms。

可见这个芯片的处理能力真的很弱。



其实当前webclient帮我做的事情并不多。

我可以把http，完成用tcp的方式来编程。

因为所有数据都是我手动拼出来的。

返回内容也是非常确定的，就是json。所以回复的解析都可以非常简单，直接找大括号就好了。

socket就用阻塞模式。

我就2个线程，一个send，一个recv。当recv返回0的时候，就说明socket被服务端断掉了。

然后我就可以重新再建立socket。

可以在Linux上测试一下。



先看opus编码数据不能识别的问题。

发现是每次得到的编码数据，发送之前，还需要加上8个字节的头部。

4个字节的长度，4个字节的enc_final_range。这个具体是什么不清楚。但是有接口可以get出来。

先试一下。

加上就可以了。



现在内存确实太紧张了。

看看opus用了多少内存。

使用了opus之后，内存还剩20K。不用opus，内存还剩200K。

amr压缩比，从网上看是非常高的。



发现opus的，没有明确写到ld脚本里。

加入再看看内存使用情况。

加入后，现在内存还剩下180K。还好。



从抓包图灵的其他机器来看，使用的就是http。所以我手动通过socket方式来做，存在可行性。

如果是https，那我就没招了。只能用现成的库。



接收的关键是先拿到这个长度。

```
Content-Length: 9593
```



webclient，采取先收取一行的方法，具体就是一个字节一个字节地收。

这个稳妥，但是效率低了点。



发送的时候，header和body要分开吗？

一起发送吧。虽然生成上会麻烦一些。

会有问题。

连接会被服务端reset掉。

放弃自己写的http。

还是在webclient上改。



在Linux试了一下我自己写的http库连接图灵的，没有问题。

所以再次尝试一下放到板端运行看看。

可以的。

所以还是把我的http库放进来用。我的库可以有效利用长连接。

我的http库可以正常工作了。

现在看返回的json。里面有好几个url。

```
{
	"code": 20007,
	"asr": "播放音乐",
	"tts": "好的，我要开始唱摇啊摇这首歌啦",
	"nlp": ["http://turing-iot.oss-cn-beijing.aliyuncs.com/tts/tts-a1d84bcae1c848f892bcfe0effe9d5e8-777815ef3611488db61b7a7b915b76d9.mp3"],
	"token": "7f542847a5224164bc5f1cfb242bc7e1",
	"func": {
		"duration": 0,
		"operate": 1000,
		"singer": "",
		"originalTitle": "",
		"originalSinger": "",
		"isPlay": 1,
		"tip": "http://turing-iot.oss-cn-beijing.aliyuncs.com/tts/tts-a1d84bcae1c848f892bcfe0effe9d5e8-1bfaaa155cf04aaa931e07540d0d7fe5.mp3",
		"id": 109201,
		"title": "摇啊摇",
		"url": "http://iot-cdn.tuling123.com/202008081311/f5573ed2a69833b126adee1e0c92d52d/media/audio/20180524/a3f6e3c80f0e498c8a83b019fba9d3e9.mp3"
	},
	"emotion": 0
}
```

有3个：

1、nlp。这个是tts的语音播报。

2、tip。这个是播放之前报一下歌名。”摇啊摇“。

3、url。这个就是歌曲的url。

所以这3个音频，要依次播放。

都用urlplayer来播放。

所以UrlPlayer需要加一个队列。

另外有个问题，就是歌曲的url。因为长度不确定。

给固定的 数组，实在有点浪费。

所以还是用动态分配吧。注意处理好。避免内存泄漏。

所以还是需要在playerwrapper里，放一个当前播放的url。

所有的url，都用动态分配的方式，在播放器回调函数里，进行url的释放操作。



现在是收到了中间的结果，也进行处理了。

```
b'[D][_play][20][26078]: play `\xdb"'
[D][_play][22][26078]: ---------strdup ptr:0x21d5f0--------
b'[PLAYER_INFO] request to play : `\xdb"'
xplayer thread dealing with msg.messageId: 0x101
[D][_player_callback][44][26094]: media is prepared. play media now.
xplayer thread dealing with msg.messageId: 0x105
[PLAYER_ERR] start() return fail.
```

清空一下buffer。

现在3个文件，播放前面2个都是失败的。

每次都是如此。为什么？

前面2个文件都是在阿里云上的。

```
ERROR : httpStream : send error.
ERROR : httpStream : xxx CdxHttpSendRequest failed.
ERROR : httpStream : Start http stream failed.
ERROR : awplayer : open stream fail, uri(http://turing-iot.oss-cn-beijing.aliyuncs.com/tts/tts-a1d84bcae1c848f892bcfe0effe9d5e8-024ea4c1ed32451990f8779dbd3afa10.mp3)
```

这个链接没有问题，在电脑上可以播放。

出错的时候，再尝试几次看看。

知道原因了，因为我判断正在播放不对，playing、prepareing，prepared，应该这三种都算播放中的。

把开机提示音加上。



# 本地vad

全志给了一个基于webrtc的vad检测库。

试了一下，16bit，单声道，如果数据buffer为1280字节，则处理函数报错。640则正常。320也正常。

处理函数不是阻塞的。

环境噪音也会触发vad识别。这个确实比较难以界定。

只有一个参数看一看调，就是set_mode，可以设置0到3,3是最严格的。

为3也有很多误识别。





# 配网方式

支持三种配网方式。

蓝牙、声波、



# mqtt

MQTT数据通道主要用于图灵IOT云端服务向设备端推送消息，

包括家长端H5资源页音频等消息的推送、微信公众号语音/文字信息推送、绑定解绑消息状态推送、设备状态上报等HTTP接口请求数据的推送等。

目前支持三种类型消息：音乐，故事，对讲。

建议设备联网成功后，立即建立与MQTT服务器的连接，以防推送的消息丢失。

**如果有使用mqtt心跳的话，建议1分钟请求一次（不要太频繁），连续三到五次没有收到回复断开链接后重新连接。**



参考资料

1、官网

http://docs.turingos.cn/