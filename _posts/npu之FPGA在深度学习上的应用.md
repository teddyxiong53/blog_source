---
title: npu之FPGA在深度学习上的应用
date: 2021-10-20 14:08:33
tags:
	- npu

---

--

AI芯片方案市场一直在持续增长， 2020年的市场规模为76亿美元，到2026年有望增长至578亿美元。在各超专业方案之间，有着不同的6先进AI硬件，例如：

- 高度定制的ASIC和SoC
- 可编程FPGA方案
- 通用GPU和CPU

通用GPU和CPU通常遵循冯·诺依曼（von Neumann）架构，其中指令提取不能与数据操作同时发生，这样，指令只能被顺序执行。

在矢量CPU和多核GPU等多处理器方案中，在某种程度上绕过了这种顺序性，

但却需要更多的跨核数据共享而增加了延迟。

这种由软件管理的并行机制必须在各处理单元之间最佳地分配工作量，否则可能会导致计算负载和通信不平衡——这种特性很难支撑自定义数据类型和特定的硬件优化。

**就延迟、功耗、并行处理和灵活/可重构性的效率而言，FPGA本质上优于GPU。**

首先， CPU和GPU必须以特定方式（如，SIMD、SIMT执行模型）处理数据，但FPGA和ASIC本质上直接在硬件中实现软件算法，逻辑单元可以简单地完成软件指令。

此外，就完成相同质量的工作而言， **FPGA功耗更低、可重构性更好**——与硬件已固化的ASIC、SoC、GPU和CPU相比，人们可以在硬件层级来更改数据流的性质。

就流行的AI芯片方案而言，ASIC领先，FPGA随后。

但是，就边缘智能计算的主要关注点而言，ASIC相形见绌。

对于成本而言尤其如此：IoT的部署数量，可能在数十个到数十万个节点之间。

众所周知，打造一款ASIC殊非易事，需要数年时间，而仅生产制造一项就需要数千万美元的巨额资本支出——通常，只有数百万至数十亿片的批量，此符合开发ASIC的成本效益。

此外，人工智能的发展日新月异。仅在几个月内，数百种现有拓扑及其各自的神经网络就会有显着的改良。随着时间的流逝，会出现具有不同功能和层级的新模型，任何公司都会希望拥抱这些变化。这就吁求一种可快速原型化和部署的低成本、灵活、可重构的平台。



新一轮的AI热潮对芯片提出了更高要求，不过，AI芯片的定义还没有严格和公认的标准。因此，可以运行深度学习算法的CPU、GPU以及FPGA和ASIC都可以被称为AI芯片。虽然都称为AI芯片，但在2019年AI落地的大背景下，AI芯片的效率更值得关注。

那么，在边缘端，FPGA能与专为边缘AI设计的ASIC共同推动AI的普及吗？





参考资料

1、

https://www.eet-china.com/news/202105200902.html

2、

https://www.leiphone.com/category/chips/ugckm1Vjn3N0v5g3.html

3、基于FPGA边缘识别算法的Verilog代码实现 灰度值转换 3*3矩阵生成 Sobel算法实现

https://www.codenong.com/cs106658899/