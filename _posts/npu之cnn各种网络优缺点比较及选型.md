---
title: npu之cnn各种网络优缺点比较及选型
date: 2021-09-03 10:46:33
tags:
	- npu

---

--

| 名字      | 基本信息                                                     | 优点                                                         | 缺点                                                         |
| --------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Alexnet   | 2012年的分类冠军，掀起来深度学习的热潮                       |                                                              | 此网络存在一个问题，不同特征图分别计算然后融合，如此卷积核只与某一部分的特征图进行卷积，这样卷积核获得的特征只有一部分，模型泛化能力有所下降，为解决这个问题，就有了shuffleNet. |
| shuffeNet | 称为压缩网络，号称是可以在移动设备上运行的网络，基于resnet改进 |                                                              |                                                              |
| VGG       | vgg主要创新为加深了网络，将5x5卷积改成两个3x3卷积，这样可以大大减少计算量。 |                                                              |                                                              |
| inception | 2014分类冠军                                                 |                                                              |                                                              |
| resnet    | 2015分类冠军                                                 | 改革性成果，如此就可以训练更深的网络，解决越深就会效果越差的问题，其中有很多resnet的改进，具体代表为，densenet,ResReXt,wide resnet. |                                                              |
| MobieNet  | 卷积操作是否必须同时考虑通道与区域，此篇论文就解决了这个问题 |                                                              |                                                              |
| SEnet     | senet为2017年分类冠军。                                      |                                                              |                                                              |
|           |                                                              |                                                              |                                                              |
|           |                                                              |                                                              |                                                              |



我们通过对 ImageNet 上训练出来的模型（如CaffeNet,VGGNet,ResNet) 进行微调，然后应用到我们自己的数据集上。

ZFNet实际上是微调（fine-tuning）了的AlexNet，并通过反卷积（Deconvolution）的方式可视化各层的输出特征图，进一步解释了卷积操作在大型网络中效果显著的原因。

通常增加网络深度可以提高准确率，但同时会牺牲一些速度和内存。但深度不是盲目堆起来的，一定要在浅层网络有一定效果的基础上，增加深度。深度增加是为了增加模型的准确率，如果浅层都学不到东西，深了也没效果。



自行搭建网络的几个技巧

1. 通道数的数量取2^n；
2. 每次MaxPooling之后通道数*2；
3. 最后一层Feature Map的尺寸不宜太大也不宜太小。
4. 输出层和Flatten（）层往往需要加最少一个隐层用于过渡特征；
5. 根据计算Flatten（）层的节点数量设计隐层节点的个数；

或者使用经典模型：

- LeNet
- AlexNet
- VGG-16/VGG-19
- GoogLeNet
- Residual Net
- SENet
- DenseNet

选择模型的技巧：根据任务的复杂度选择最合适的模型，简单任务可以选择VGG等比较浅的网络，目前复杂任务主流的选择是比较深的Residual Net



迁移学习是将训练好的任务A（最常用的是ImageNet）的模型用于当前任务的网络的初始化，然后在自己的数据上进行微调。该方法在数据集比较小的任务上往往效果很好。



数据预处理

1、导入包，numpy和pandas。numpy用来计算，pandas用来导入和管理数据集。

2、导入数据集。一般是csv格式。

3、处理丢失数据。数据很少是完整的。

4、解析分类数据。是指把非数值类型的转成数值，例如yes、no这些。

5、拆分数据集。

6、特征缩放。



从模型的发展过程中，随着准确率的提高，网络结构也在不断的进行改进，

现在主要是两个方向，一是深度，二是复杂度。

此外还有卷积核的变换等等。

大家发表的paper一般可以分为两大类，一类是网络结构的改进，一类是训练过程的改进，如droppath，loss改进等。



之后网络结构设计发展主要有两条主线，

一条是Inception系列（即上面说的复杂度），从GoogLeNet 到Inception V2 V3 V4，Inception ResNet。 Inception module模块在不断变化，

一条是VGG系列（即深度），用简单的结构，尽可能的使得网络变得更深。从VGG 发展到ResNet ，再到DenseNet ，DPN等。



| 模型名                       | AlexNet        | ZFNet          | VGG            | GoogLeNet | ResNet |
| ---------------------------- | -------------- | -------------- | -------------- | --------- | ------ |
| 初入江湖                     | 2012           | 2013           | 2014           | 2014      | 2015   |
| 层数                         | 8              | 8              | 19             | 22        | 152    |
| Top-5错误                    | 16.4%          | 11.2%          | 7.3%           | 6.7%      | 3.57%  |
| Data Augmentation            | +              | +              | +              | +         | +      |
| Inception(NIN)               | –              | –              | –              | +         | –      |
| 卷积层数                     | 5              | 5              | 16             | 21        | 151    |
| 卷积核大小                   | 11,5,3         | 7,5,3          | 3              | 7,1,3,5   | 7,1,3  |
| 全连接层数                   | 3              | 3              | 3              | 1         | 1      |
| 全连接层大小                 | 4096,4096,1000 | 4096,4096,1000 | 4096,4096,1000 | 1000      | 1000   |
| Dropout                      | +              | +              | +              | +         | +      |
| Local Response Normalization | +              | +              | –              | +         | –      |
| Batch Normalization          | –              | –              | –              | –         | +      |





当数据文件过大时，我们不能将数据直接全部加载到内存，这是需要Keras的**生成器**机制

**生成器**的作用是在训练的过程中不停的给模型喂数据，同时保证内存中只保存少量的数据

keras中使用**ImageDataGenerator**定义生成器的类

ImageDataGenerator里面的参数主要用于数据扩充（数据增强），之后会讲解数据增强的作用



我们要使用的参数

**batch_size**：取出每个batch中图片的个数，图片越少，计算越快，显存消耗越低，但是模型训练过程容易不稳定。常见的策略是从2^n中选择满足显存需求的最大的值

**target_size**：batch中图像的尺寸，当前数据集不需要resize，直接选择输入图片的尺寸(66, 66)即可

**class_mode**：默认是categorical，即当前数据是多类数据，如果实二分类，其值应该是‘binary’



自创模型：

1. 堆积卷积操作（Conv2D）和最大池化操作（MaxPooling2D），第一层需要指定输入图像的尺寸和通道数
2. Flatten()用于将Feature Map展开成特征向量
3. 之后接全连接层和激活层，注意多分类应该使用softmax激活函数



通过上面的分析，我们发现模型严重过拟合了，我们可以采用以下策略减轻过拟合的问题：

1. Dropout
2. 早停
3. 正则化
4. 数据增强

一些经验：

1. Dropout放到后面几层，尤其是全连接层；
2. 丢失率为0.25效果往往比较好。

Dropout太少或者丢失率太小的话不能有效缓解过拟合，Dropout太多或者丢失率太大容易导致模型不收敛。

没有太固定的套路决定Dropout怎么设置，更好的策略是根据验证机调整



数据扩充的几点思考：

1. 扩充策略的设置要建立在对数据集充分的观测和理解上；
2. 正确的扩充策略能增加样本数量，大幅减轻过拟合的问题；
3. 错误的扩充策略很有可能导致模型不好收敛，更严重的问题是使训练集和测试集的分布更加不一致，加剧过拟合的问题；
4. 往往开发者需要根据业务场景自行实现扩充策略



迁移学习是将训练好的任务A（最常用的是ImageNet）的模型用于当前任务的网络的初始化，然后在自己的数据上进行微调。该方法在数据集比较小的任务上往往效果很好。

```
keras.applications.vgg16.VGG16(include_top=True, 
            weights='imagenet', 
            input_tensor=None, 
            input_shape=None, 
            pooling=None, 
            classes=1000)
```

**weights**：选择初始化的数据集，执行时会自动将模型文件下载到本地，None表示随机初始化；

**include_top**：由于我们的输出层和imageNet不一样，所以应该选择False；

**input_shape**：输入图像的尺寸，建议和网络结构相同（VGG16是224），所以我们需要更改生成器的参数值。

Keras提供用户自定义迁移学习时哪些层可以微调，哪些层不需要微调，通过layer.trainable设置

Keras使用迁移学习提供的模型往往比较深，容易产生梯度消失或者梯度爆炸的问题，建议添加BN层

最好的策略是选择好适合自己任务的网络后自己使用ImageNet数据集进行训练







# 参考资料

1、CNN网络模型发展进程及各个网络优缺点

https://blog.csdn.net/comway_Li/article/details/82729920

2、

https://www.bilibili.com/read/cv8103622

3、

https://zhuanlan.zhihu.com/p/52622447

4、

https://blog.csdn.net/weixin_41809530/article/details/108195586

5、轻量级CNN架构设计

https://zhuanlan.zhihu.com/p/320447427

6、

https://github.com/senliuy/12306_crack

7、

https://github.com/weslynn/AlphaTree-graphic-deep-neural-network