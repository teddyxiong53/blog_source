---
title: npu之sklearn
date: 2021-08-27 19:20:33
tags:
	- npu

---

--

**Scikit-learn(sklearn)的定位是通用机器学习库**，

而**TensorFlow(tf)的定位主要是深度学习库**。

一个显而易见的不同：

tf并未提供sklearn那种强大的特征工程，

如维度压缩、特征选择等。

究其根本，我认为是因为机器学习模型的两种不同的处理数据的方式：

- **传统机器学习：利用特征工程(feature engineering)，人为对数据进行提炼清洗**
- **深度学习：利用表示学习(representation learning)，机器学习模型自身对数据进行提炼**



**sklearn更倾向于使用者可以自行对数据进行处理**，

比如选择特征、压缩维度、转换格式，是传统机器学习库。

而以tf为代表的深度学习库会自动从数据中抽取有效特征，

而不需要人为的来做这件事情，因此并未提供类似的功能。



**sklearn 中的模块都是高度抽象化的，**

**所有的分类器基本都可以在3-5行内完成，**

**所有的转换器(如scaler和transformer)也都有固定的格式**。

这种抽 象化限制了使用者的自由度，

但增加了模型的效率，降低了批量化、标准化的的难度(通过使用pipeline)。



**而tf不同，虽然是深度学习库，但它有很高的自由度**。

你依然可以用它做传统机器学习所做的事情，

代价是你需要自己实现算法。

因此用tf类比sklearn不适合，

封装在tf等工具库上的keras才更像深度学习界的sklearn。

从自由度角度来看，tf更高；

从抽象化、封装程度来看，sklearn更高；

从易用性角度来看，sklearn更高。



sklearn主要适合中小型的、实用机器学习项目，

尤其是那种数据量不大且需要使用者手动对数据进行处理，

并选择合适模型的项目。

这类项目往往在CPU上就可以完成，对硬件要求低。

tf主要适合已经明确了解需要用深度学习，

且数据处理需求不高的项目。

这类项目往往数据量较大，且最终需要的精度更高，一般都需要GPU加速运算。

对于深度学习做“小样”可以在采样的小数据集上用keras做快速的实验，

没了解的过朋友看一下keras的示例代码，就可以了解为什么keras堪比深度学习上的sklearn了。



不过sklearn 还是很有 必要学习的

理论上来说，

**深度学习技术也是机器学习的一个组成部分**，

学习其他传统机器学习方法对深入理解深度学习技术有很大帮助，

知道模型凸的条件，才能更好的理解神 经网络的非凸。

知道传统模型的优点，才能更好的理解深度学习并不是万能的，

也有很多问题和场景直接使用深度学习方法会遇到瓶颈和问题，需要传统方法来解 决。



更常见的情况下，可以把**sklearn和tf，甚至keras结合起来使用**。

sklearn肩负基本的数据清理任务，

keras用于对问题进行小规模实验验证想法，

而tf用于在完整的的数据上进行严肃的调参(炼丹)任务。



而单独把sklearn拿出来看的话，它的文档做的特别好，

初学者跟着看一遍sklearn支持的功能

大概就对机器学习包括的很多内容有了基本的了解。

举个简单的例子，

sklearn很多时候对单独的知识点有概述，

比如简单的异常检测。

因此，sklearn不仅仅是简单的工具库，

它的文档更像是一份简单的新手入门指南。

因此，

以sklearn为代表的传统机器学习库（如瑞士军刀般的万能但高度抽象），

和以tf为代表的自由灵活更具有针对性的深度学习库（如乐高般高度自由但使用繁琐）

都是机器学习者必须要了解的工具。



从实践上来说，

深度学习方法一般需要大量GPU机器，

工业界哪怕大公司的GPU资源也是有限的，

一般只有深度学习方法效果远好于传统方法并且 对业务提升很大的情况下，

才会考虑使用深度学习方法，

例如语音识别，图像识别等任务现在深度学习方法用的比较多，

而NLP领域除了机器翻译以外，

其他大部 分任务仍然更常使用传统方法。

**传统方法一般有着更好的可解释性，**

这对检查调试模型也是非常有帮助的。

工业上一般喜欢招能解决问题的人，

而不是掌握最火技术 的人，因此在了解深度学习技术的同时，学习一下传统方法是很有好处的。





参考资料

1、sklearn 和tensorflow的区别

https://blog.csdn.net/youhuakongzhi/article/details/94208335